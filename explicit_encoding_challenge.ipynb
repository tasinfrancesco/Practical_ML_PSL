{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "name": "explicit_encoding_challenge.ipynb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a33eb10ec74b412d8b2d4e69c016afaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c2b383a55a44a0b649eccd6b4bfb62",
              "IPY_MODEL_af4573e9ba034627a2f088506a1138d9",
              "IPY_MODEL_2c1dd48c2e6d49898491e7d4370767a9"
            ],
            "layout": "IPY_MODEL_0c837315f12f49a382fc5c800728f85c"
          }
        },
        "e0c2b383a55a44a0b649eccd6b4bfb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57e3a003b75a423dbfac0b983995d656",
            "placeholder": "​",
            "style": "IPY_MODEL_ab47b701f1e34a418a8d1ab85ed81422",
            "value": "model.safetensors: 100%"
          }
        },
        "af4573e9ba034627a2f088506a1138d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76fedc6634aa452897a05e8fd2b4f56b",
            "max": 596091758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3ec03d8d36a40d9a64046d9423fabd8",
            "value": 596091758
          }
        },
        "2c1dd48c2e6d49898491e7d4370767a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d026e73714b34da6910c128efdbb2f1c",
            "placeholder": "​",
            "style": "IPY_MODEL_fba9fd00902c49bb9869c0247de5de70",
            "value": " 596M/596M [00:09&lt;00:00, 91.9MB/s]"
          }
        },
        "0c837315f12f49a382fc5c800728f85c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57e3a003b75a423dbfac0b983995d656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab47b701f1e34a418a8d1ab85ed81422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76fedc6634aa452897a05e8fd2b4f56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ec03d8d36a40d9a64046d9423fabd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d026e73714b34da6910c128efdbb2f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba9fd00902c49bb9869c0247de5de70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3234422bf7b84c209ecd580012d62b1c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5a411ca8c52d477499b065029f32a983",
              "IPY_MODEL_6696b984904f41eca3a6dd246655f80d",
              "IPY_MODEL_7cd861dada1947e1b0b666c090fc97a5"
            ],
            "layout": "IPY_MODEL_80160ec329bb445f8d325aecb69a595d"
          }
        },
        "5a411ca8c52d477499b065029f32a983": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0609a23a40694d9a891e6115998a9181",
            "placeholder": "​",
            "style": "IPY_MODEL_d667f53ba8c84df89b52f2a46f178ab2",
            "value": " 50%"
          }
        },
        "6696b984904f41eca3a6dd246655f80d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a2755662f50f4e4892499dde78d81760",
            "max": 24,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_191bc4ea0c8f4468879761c47857df32",
            "value": 12
          }
        },
        "7cd861dada1947e1b0b666c090fc97a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_37890620ac764cdfbc081322158a0905",
            "placeholder": "​",
            "style": "IPY_MODEL_799ecb12b537415ea1a2370801bb45d0",
            "value": " 12/24 [05:34&lt;06:08, 30.69s/it]"
          }
        },
        "80160ec329bb445f8d325aecb69a595d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0609a23a40694d9a891e6115998a9181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d667f53ba8c84df89b52f2a46f178ab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a2755662f50f4e4892499dde78d81760": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "191bc4ea0c8f4468879761c47857df32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "37890620ac764cdfbc081322158a0905": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "799ecb12b537415ea1a2370801bb45d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasinfrancesco/Practical_ML_PSL/blob/main/explicit_encoding_challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ds6qDsKCeG_W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strptime\n",
        "df = pd.read_csv(\"train.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.describe())\n",
        "print(df.head(5))\n",
        "# dt = pd.read_csv(\"test.csv\")\n",
        "# print(dt[[\"UTC\" not in cd for cd in dt[\"creation_date\"]]])\n",
        "type(df[\"creation_date\"][0])\n",
        "print(strptime(df[\"creation_date\"][0], \"%Y-%m-%d %H:%M:%S.%f %Z\"))\n",
        "\n",
        "print(strptime(df[\"creation_date\"][1], \"%Y-%m-%d %H:%M:%S.%f %Z\"))\n",
        "print(df[\"creation_date\"][0])\n",
        "print(df[\"creation_date\"][1])\n",
        "\n",
        "def timeinfo(timestr):\n",
        "  time_struct = strptime(df[\"creation_date\"][0], \"%Y-%m-%d %H:%M:%S.%f %Z\")\n",
        "\n",
        "  #is it a weekend\n",
        "  if time_struct.tm_wday >= 5:\n",
        "    weekend = 1\n",
        "  else:\n",
        "    weekend = 0\n",
        "\n",
        "  #is it a vacation\n",
        "\n"
      ],
      "metadata": {
        "id": "BqafgUcieSGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideas for feature engineering:\n",
        "\n",
        "  - local comment frequency (estimated from change in post_id over time window)\n",
        "  - is it a vacation\n",
        "  - is it a weekend\n",
        "\n",
        "post\n",
        "\n",
        "  - post length\n",
        "  - positivity of content?\n",
        "  - politeness?\n",
        "\n",
        "BERT:\n",
        "  - encode in some way politeness? (there's a stanford thingy)\n",
        "  - CodeBERT vs BERT?\n",
        "    - Padding and truncation (how long?)\n",
        "  - hide vectors of code/cut it out for text processing?\n",
        "  - clarity as:\n",
        "    - readability\n",
        "    - sentence/word length\n",
        "    - coding depth (nesting)\n",
        "    - number of questions\n",
        "    - presence of errors\n",
        "    - number of topics mentioned (\"I tried...\")\n",
        "\n",
        "User ID?\n",
        "\n",
        "  - is it better to predict for a given user (average) or for single posts?\n",
        "\n",
        "text vectorisation:\n",
        "\n",
        "analysis:\n",
        "\n",
        "  - logistic regression (not great given collinearity)\n",
        "  - random trees?\n",
        "\n",
        "predict score, then guess target (not target directly)\n",
        "\n",
        "Whether the output includes both the code and the error? (not applicable to everything though)\n"
      ],
      "metadata": {
        "id": "NW_2G-4HeXDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentencepiece\n",
        "# !pip install protobuf==3.20.0\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lanwuwei/BERTOverflow_stackoverflow_github\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"lanwuwei/BERTOverflow_stackoverflow_github\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "a33eb10ec74b412d8b2d4e69c016afaf",
            "e0c2b383a55a44a0b649eccd6b4bfb62",
            "af4573e9ba034627a2f088506a1138d9",
            "2c1dd48c2e6d49898491e7d4370767a9",
            "0c837315f12f49a382fc5c800728f85c",
            "57e3a003b75a423dbfac0b983995d656",
            "ab47b701f1e34a418a8d1ab85ed81422",
            "76fedc6634aa452897a05e8fd2b4f56b",
            "e3ec03d8d36a40d9a64046d9423fabd8",
            "d026e73714b34da6910c128efdbb2f1c",
            "fba9fd00902c49bb9869c0247de5de70"
          ]
        },
        "id": "Hv9DzrqceUnl",
        "outputId": "0b6017b5-672d-4c29-f33a-d3f8b234b5de"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/596M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a33eb10ec74b412d8b2d4e69c016afaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at lanwuwei/BERTOverflow_stackoverflow_github and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_1_to_10 = []\n",
        "cls = [tokenizer.cls_token_id]\n",
        "sep = [tokenizer.sep_token_id]\n",
        "for t in df[\"text\"][0:10]:\n",
        "  tok = tokenizer.tokenize(t)\n",
        "  ids = tokenizer.convert_tokens_to_ids(tok)\n",
        "  ids_special_tokens = cls + ids + sep\n",
        "\n",
        "  decoded_str = tokenizer.decode(ids_special_tokens)\n",
        "\n",
        "  print(\"start:               \", t)\n",
        "  print(\"tokenize:            \", tok)\n",
        "  print(\"convert_tokens_to_ids\", ids)\n",
        "  print(\"add special tokens:  \", ids_special_tokens)\n",
        "  print(\"================\")\n",
        "  print(\"decode:              \", decoded_str)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA2LHWA83Dsr",
        "outputId": "81d890ed-8874-4447-807c-f5875c343808",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start:                Sincerely appreciate your taking a look. I never have to query across partitions. Disk space is not an issue. The volume of data definitely justifies the partitioning. Separate tables and or databases are not the option. Still my question remains unanswered :-( Many thanks in advance.\n",
            "tokenize:             ['Sincerely', 'appreciate', 'your', 'taking', 'a', 'look', '.', 'I', 'never', 'have', 'to', 'query', 'across', 'partitions', '.', 'Disk', 'space', 'is', 'not', 'an', 'issue', '.', 'The', 'volume', 'of', 'data', 'definitely', 'justifies', 'the', 'partitioning', '.', 'Separate', 'tables', 'and', 'or', 'databases', 'are', 'not', 'the', 'option', '.', 'Still', 'my', 'question', 'remains', 'unanswered', ':', '-', '(', 'Many', 'thanks', 'in', 'advance', '.']\n",
            "convert_tokens_to_ids [52297, 5429, 1881, 4670, 69, 2076, 18, 45, 3071, 1851, 1790, 2400, 4036, 12207, 18, 17326, 3257, 1804, 1859, 1794, 2456, 18, 1911, 7817, 1815, 1943, 5480, 57754, 1783, 14744, 18, 21624, 3133, 1809, 1876, 5494, 1894, 1859, 1783, 2685, 18, 8020, 1887, 2318, 6989, 23991, 30, 17, 12, 6341, 4453, 1797, 4152, 18]\n",
            "add special tokens:   [2, 52297, 5429, 1881, 4670, 69, 2076, 18, 45, 3071, 1851, 1790, 2400, 4036, 12207, 18, 17326, 3257, 1804, 1859, 1794, 2456, 18, 1911, 7817, 1815, 1943, 5480, 57754, 1783, 14744, 18, 21624, 3133, 1809, 1876, 5494, 1894, 1859, 1783, 2685, 18, 8020, 1887, 2318, 6989, 23991, 30, 17, 12, 6341, 4453, 1797, 4152, 18, 4]\n",
            "================\n",
            "decode:               [CLS] Sincerely appreciate your taking a look. I never have to query across partitions. Disk space is not an issue. The volume of data definitely justifies the partitioning. Separate tables and or databases are not the option. Still my question remains unanswered : - ( Many thanks in advance. [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                That's not what I'm doing right now ?\n",
            "The MapRoute provide the Markers to the MapController, but my question is more about how to handle interaction between the filters and the map.\n",
            "I'm going to take a look at the components, sounds good!\n",
            "tokenize:             ['That', \"'\", 's', 'not', 'what', 'I', \"'\", 'm', 'doing', 'right', 'now', '?', 'The', 'MapRoute', 'provide', 'the', 'Markers', 'to', 'the', 'Map', '##Controller', ',', 'but', 'my', 'question', 'is', 'more', 'about', 'how', 'to', 'handle', 'interaction', 'between', 'the', 'filters', 'and', 'the', 'map', '.', 'I', \"'\", 'm', 'going', 'to', 'take', 'a', 'look', 'at', 'the', 'components', ',', 'sounds', 'good', '!']\n",
            "convert_tokens_to_ids [2693, 11, 87, 1859, 2015, 45, 11, 81, 2503, 2424, 2389, 35, 1911, 50672, 3202, 1783, 33534, 1790, 1783, 5319, 3316, 16, 1868, 1887, 2318, 1804, 2122, 2201, 1995, 1790, 3043, 7219, 2569, 1783, 6520, 1809, 1783, 2901, 18, 45, 11, 81, 2774, 1790, 2725, 69, 2076, 1946, 1783, 4378, 16, 5125, 2565, 5]\n",
            "add special tokens:   [2, 2693, 11, 87, 1859, 2015, 45, 11, 81, 2503, 2424, 2389, 35, 1911, 50672, 3202, 1783, 33534, 1790, 1783, 5319, 3316, 16, 1868, 1887, 2318, 1804, 2122, 2201, 1995, 1790, 3043, 7219, 2569, 1783, 6520, 1809, 1783, 2901, 18, 45, 11, 81, 2774, 1790, 2725, 69, 2076, 1946, 1783, 4378, 16, 5125, 2565, 5, 4]\n",
            "================\n",
            "decode:               [CLS] That's not what I'm doing right now? The MapRoute provide the Markers to the MapController, but my question is more about how to handle interaction between the filters and the map. I'm going to take a look at the components, sounds good! [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                `/include/ssp/stdio.h` check the contents of that file. The path is strange for starters\n",
            "tokenize:             ['`', '/', 'include', '/', 'ssp', '/', 'stdio', '.', 'h', '`', 'check', 'the', 'contents', 'of', 'that', 'file', '.', 'The', 'path', 'is', 'strange', 'for', 'starters']\n",
            "convert_tokens_to_ids [68, 19, 3005, 19, 52731, 19, 19080, 18, 76, 68, 2175, 1783, 3881, 1815, 1826, 1973, 18, 1911, 2643, 1804, 5022, 1829, 17596]\n",
            "add special tokens:   [2, 68, 19, 3005, 19, 52731, 19, 19080, 18, 76, 68, 2175, 1783, 3881, 1815, 1826, 1973, 18, 1911, 2643, 1804, 5022, 1829, 17596, 4]\n",
            "================\n",
            "decode:               [CLS] ` / include / ssp / stdio. h ` check the contents of that file. The path is strange for starters [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                \"At each step you multiply the value of the filter 'pixel' by the corresponding value of the image pixel which is under that particular filter 'pixel' (the 9 pixels under the filter are all affected)\" I found this statement in a lot of material on convolution. I previously asked a question about this in SO and I find this to be a bit ambiguous, do we need to multiply the pixel value by the filter or take the intensity of the pixel color and then multiply the filter like in my code. if u have a non balanced filter I found that it leads to bleeding effect like the greens would be effecting red's\n",
            "tokenize:             ['\"', 'At', 'each', 'step', 'you', 'multiply', 'the', 'value', 'of', 'the', 'filter', \"'\", 'pixel', \"'\", 'by', 'the', 'corresponding', 'value', 'of', 'the', 'image', 'pixel', 'which', 'is', 'under', 'that', 'particular', 'filter', \"'\", 'pixel', \"'\", '(', 'the', '9', 'pixels', 'under', 'the', 'filter', 'are', 'all', 'affected', ')', '\"', 'I', 'found', 'this', 'statement', 'in', 'a', 'lot', 'of', 'material', 'on', 'convolution', '.', 'I', 'previously', 'asked', 'a', 'question', 'about', 'this', 'in', 'SO', 'and', 'I', 'find', 'this', 'to', 'be', 'a', 'bit', 'ambiguous', ',', 'do', 'we', 'need', 'to', 'multiply', 'the', 'pixel', 'value', 'by', 'the', 'filter', 'or', 'take', 'the', 'intensity', 'of', 'the', 'pixel', 'color', 'and', 'then', 'multiply', 'the', 'filter', 'like', 'in', 'my', 'code', '.', 'if', 'u', 'have', 'a', 'non', 'balanced', 'filter', 'I', 'found', 'that', 'it', 'leads', 'to', 'bleeding', 'effect', 'like', 'the', 'green', '##s', 'would', 'be', 'effecting', 'red', \"'\", 's']\n",
            "convert_tokens_to_ids [6, 3450, 2184, 2974, 1807, 9174, 1783, 2116, 1815, 1783, 3079, 11, 4393, 11, 1956, 1783, 4608, 2116, 1815, 1783, 2347, 4393, 1945, 1804, 2348, 1826, 3240, 3079, 11, 4393, 11, 12, 1783, 29, 5817, 2348, 1783, 3079, 1894, 1935, 8596, 13, 6, 45, 2447, 1832, 3096, 1797, 69, 2751, 1815, 7853, 1824, 19438, 18, 45, 5861, 4872, 69, 2318, 2201, 1832, 1797, 4165, 1809, 45, 2202, 1832, 1790, 1827, 69, 2504, 12096, 16, 1845, 1955, 1941, 1790, 9174, 1783, 4393, 2116, 1956, 1783, 3079, 1876, 2725, 1783, 21408, 1815, 1783, 4393, 3371, 1809, 1991, 9174, 1783, 3079, 1923, 1797, 1887, 1925, 18, 1909, 89, 1851, 69, 3019, 14776, 3079, 45, 2447, 1826, 1813, 7475, 1790, 32843, 3372, 1923, 1783, 6778, 1016, 1961, 1827, 31499, 2646, 11, 87]\n",
            "add special tokens:   [2, 6, 3450, 2184, 2974, 1807, 9174, 1783, 2116, 1815, 1783, 3079, 11, 4393, 11, 1956, 1783, 4608, 2116, 1815, 1783, 2347, 4393, 1945, 1804, 2348, 1826, 3240, 3079, 11, 4393, 11, 12, 1783, 29, 5817, 2348, 1783, 3079, 1894, 1935, 8596, 13, 6, 45, 2447, 1832, 3096, 1797, 69, 2751, 1815, 7853, 1824, 19438, 18, 45, 5861, 4872, 69, 2318, 2201, 1832, 1797, 4165, 1809, 45, 2202, 1832, 1790, 1827, 69, 2504, 12096, 16, 1845, 1955, 1941, 1790, 9174, 1783, 4393, 2116, 1956, 1783, 3079, 1876, 2725, 1783, 21408, 1815, 1783, 4393, 3371, 1809, 1991, 9174, 1783, 3079, 1923, 1797, 1887, 1925, 18, 1909, 89, 1851, 69, 3019, 14776, 3079, 45, 2447, 1826, 1813, 7475, 1790, 32843, 3372, 1923, 1783, 6778, 1016, 1961, 1827, 31499, 2646, 11, 87, 4]\n",
            "================\n",
            "decode:               [CLS] \" At each step you multiply the value of the filter'pixel'by the corresponding value of the image pixel which is under that particular filter'pixel'( the 9 pixels under the filter are all affected ) \" I found this statement in a lot of material on convolution. I previously asked a question about this in SO and I find this to be a bit ambiguous, do we need to multiply the pixel value by the filter or take the intensity of the pixel color and then multiply the filter like in my code. if u have a non balanced filter I found that it leads to bleeding effect like the greens would be effecting red's [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                Everyone suggesting STRING_SPLIT, how can this function split string into *columns* (not rows like it's intended)?\n",
            "tokenize:             ['Everyone', 'suggesting', 'STRING', '_', 'SPLIT', ',', 'how', 'can', 'this', 'function', 'split', 'string', 'into', '*', 'columns', '*', '(', 'not', 'rows', 'like', 'it', \"'\", 's', 'intended', ')', '?']\n",
            "convert_tokens_to_ids [17901, 12243, 15352, 67, 46528, 16, 1995, 1847, 1832, 2018, 4002, 2204, 2110, 14, 3175, 14, 12, 1859, 3058, 1923, 1813, 11, 87, 5787, 13, 35]\n",
            "add special tokens:   [2, 17901, 12243, 15352, 67, 46528, 16, 1995, 1847, 1832, 2018, 4002, 2204, 2110, 14, 3175, 14, 12, 1859, 3058, 1923, 1813, 11, 87, 5787, 13, 35, 4]\n",
            "================\n",
            "decode:               [CLS] Everyone suggesting STRING _ SPLIT, how can this function split string into * columns * ( not rows like it's intended )? [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                I would suggest you to put a flag there, and change that flag whenever you need.\n",
            "tokenize:             ['I', 'would', 'suggest', 'you', 'to', 'put', 'a', 'flag', 'there', ',', 'and', 'change', 'that', 'flag', 'whenever', 'you', 'need', '.']\n",
            "convert_tokens_to_ids [45, 1961, 2626, 1807, 1790, 2489, 69, 4121, 1965, 16, 1809, 2285, 1826, 4121, 4672, 1807, 1941, 18]\n",
            "add special tokens:   [2, 45, 1961, 2626, 1807, 1790, 2489, 69, 4121, 1965, 16, 1809, 2285, 1826, 4121, 4672, 1807, 1941, 18, 4]\n",
            "================\n",
            "decode:               [CLS] I would suggest you to put a flag there, and change that flag whenever you need. [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                Well, the exclusion in subqueies is documented as well, so I'd say it's   awkward, but not surprising. The reason IMO is that the whole query must guarantie to deliver identical `NEXTVAL` in all calls; which is trivial in `select seq.nextval a, seq.nextval b from dual` , but less trivial in   `select * from (select seq.nextval a from dual),(select seq.nextval b from dual)`\n",
            "tokenize:             ['Well', ',', 'the', 'exclusion', 'in', 'sub', '##que', '##ies', 'is', 'documented', 'as', 'well', ',', 'so', 'I', \"'\", 'd', 'say', 'it', \"'\", 's', 'awkward', ',', 'but', 'not', 'surprising', '.', 'The', 'reason', 'IMO', 'is', 'that', 'the', 'whole', 'query', 'must', 'guarant', '##ie', 'to', 'deliver', 'identical', '`', 'NEXT', '##VAL', '`', 'in', 'all', 'calls', ';', 'which', 'is', 'trivial', 'in', '`', 'select', 'seq', '.', 'nextval', 'a', ',', 'seq', '.', 'nextval', 'b', 'from', 'dual', '`', ',', 'but', 'less', 'trivial', 'in', '`', 'select', '*', 'from', '(', 'select', 'seq', '.', 'nextval', 'a', 'from', 'dual', ')', ',', '(', 'select', 'seq', '.', 'nextval', 'b', 'from', 'dual', ')', '`']\n",
            "convert_tokens_to_ids [4778, 16, 1783, 20020, 1797, 2307, 3153, 2119, 1804, 6986, 1856, 2542, 16, 1948, 45, 11, 72, 2620, 1813, 11, 87, 14374, 16, 1868, 1859, 11643, 18, 1911, 2691, 10706, 1804, 1826, 1783, 3289, 2400, 2743, 5047, 1940, 1790, 7717, 6178, 68, 24625, 13666, 68, 1797, 1935, 3156, 31, 1945, 1804, 6530, 1797, 68, 2357, 12772, 18, 45719, 69, 16, 12772, 18, 45719, 70, 1912, 14610, 68, 16, 1868, 3425, 6530, 1797, 68, 2357, 14, 1912, 12, 2357, 12772, 18, 45719, 69, 1912, 14610, 13, 16, 12, 2357, 12772, 18, 45719, 70, 1912, 14610, 13, 68]\n",
            "add special tokens:   [2, 4778, 16, 1783, 20020, 1797, 2307, 3153, 2119, 1804, 6986, 1856, 2542, 16, 1948, 45, 11, 72, 2620, 1813, 11, 87, 14374, 16, 1868, 1859, 11643, 18, 1911, 2691, 10706, 1804, 1826, 1783, 3289, 2400, 2743, 5047, 1940, 1790, 7717, 6178, 68, 24625, 13666, 68, 1797, 1935, 3156, 31, 1945, 1804, 6530, 1797, 68, 2357, 12772, 18, 45719, 69, 16, 12772, 18, 45719, 70, 1912, 14610, 68, 16, 1868, 3425, 6530, 1797, 68, 2357, 14, 1912, 12, 2357, 12772, 18, 45719, 69, 1912, 14610, 13, 16, 12, 2357, 12772, 18, 45719, 70, 1912, 14610, 13, 68, 4]\n",
            "================\n",
            "decode:               [CLS] Well, the exclusion in subqueies is documented as well, so I'd say it's awkward, but not surprising. The reason IMO is that the whole query must guarantie to deliver identical ` NEXTVAL ` in all calls ; which is trivial in ` select seq. nextval a, seq. nextval b from dual `, but less trivial in ` select * from ( select seq. nextval a from dual ), ( select seq. nextval b from dual ) ` [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                @NimChimpsky I have gone through that question already. It didn't help.\n",
            "tokenize:             ['@', 'Nim', '##Chimp', '##sky', 'I', 'have', 'gone', 'through', 'that', 'question', 'already', '.', 'It', 'didn', \"'\", 't', 'help', '.']\n",
            "convert_tokens_to_ids [36, 30500, 33515, 55917, 45, 1851, 6259, 2462, 1826, 2318, 2601, 18, 2049, 3186, 11, 88, 2213, 18]\n",
            "add special tokens:   [2, 36, 30500, 33515, 55917, 45, 1851, 6259, 2462, 1826, 2318, 2601, 18, 2049, 3186, 11, 88, 2213, 18, 4]\n",
            "================\n",
            "decode:               [CLS] @ NimChimpsky I have gone through that question already. It didn't help. [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                that is via Client Object Model not the Restful Api which I was asking about\n",
            "tokenize:             ['that', 'is', 'via', 'Client', 'Object', 'Model', 'not', 'the', 'Restful', 'Api', 'which', 'I', 'was', 'asking', 'about']\n",
            "convert_tokens_to_ids [1826, 1804, 2781, 5030, 3454, 4501, 1859, 1783, 26017, 10105, 1945, 45, 2078, 4465, 2201]\n",
            "add special tokens:   [2, 1826, 1804, 2781, 5030, 3454, 4501, 1859, 1783, 26017, 10105, 1945, 45, 2078, 4465, 2201, 4]\n",
            "================\n",
            "decode:               [CLS] that is via Client Object Model not the Restful Api which I was asking about [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                Don't cast the result of a call to `malloc()` (or `calloc()` or `realloc()`) - it's unnecessary and can mask a very real error if you fail to have the appropriate prototype in scope.\n",
            "tokenize:             ['Don', \"'\", 't', 'cast', 'the', 'result', 'of', 'a', 'call', 'to', '`', 'malloc', '(', ')', '`', '(', 'or', '`', 'calloc', '(', ')', '`', 'or', '`', 'realloc', '(', ')', '`', ')', '-', 'it', \"'\", 's', 'unnecessary', 'and', 'can', 'mask', 'a', 'very', 'real', 'error', 'if', 'you', 'fail', 'to', 'have', 'the', 'appropriate', 'prototype', 'in', 'scope', '.']\n",
            "convert_tokens_to_ids [4905, 11, 88, 4644, 1783, 2279, 1815, 69, 2077, 1790, 68, 7507, 12, 13, 68, 12, 1876, 68, 22967, 12, 13, 68, 1876, 68, 13762, 12, 13, 68, 13, 17, 1813, 11, 87, 7292, 1809, 1847, 7309, 69, 2441, 2984, 2093, 1909, 1807, 3607, 1790, 1851, 1783, 4116, 6264, 1797, 3773, 18]\n",
            "add special tokens:   [2, 4905, 11, 88, 4644, 1783, 2279, 1815, 69, 2077, 1790, 68, 7507, 12, 13, 68, 12, 1876, 68, 22967, 12, 13, 68, 1876, 68, 13762, 12, 13, 68, 13, 17, 1813, 11, 87, 7292, 1809, 1847, 7309, 69, 2441, 2984, 2093, 1909, 1807, 3607, 1790, 1851, 1783, 4116, 6264, 1797, 3773, 18, 4]\n",
            "================\n",
            "decode:               [CLS] Don't cast the result of a call to ` malloc ( ) ` ( or ` calloc ( ) ` or ` realloc ( ) ` ) - it's unnecessary and can mask a very real error if you fail to have the appropriate prototype in scope. [SEP]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install evaluate\n",
        "#Imports\n",
        "from torch.optim import AdamW\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strptime\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import roc_auc_score\n",
        "import evaluate\n",
        "from transformers import Trainer, AutoModelForTokenClassification, AutoModelForSequenceClassification, AutoTokenizer, get_linear_schedule_with_warmup\n",
        "from datasets import DatasetDict, load_dataset, Dataset\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import datasets\n"
      ],
      "metadata": {
        "id": "UAweK_A260gk"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip show torch"
      ],
      "metadata": {
        "id": "nJd6N1IIhOc5",
        "outputId": "6c2ddf8f-9141-4505-a49b-e6d884abf6a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Name: torch\n",
            "Version: 2.9.0+cpu\n",
            "Summary: Tensors and Dynamic neural networks in Python with strong GPU acceleration\n",
            "Home-page: https://pytorch.org\n",
            "Author: \n",
            "Author-email: PyTorch Team <packages@pytorch.org>\n",
            "License: BSD-3-Clause\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: filelock, fsspec, jinja2, networkx, setuptools, sympy, typing-extensions\n",
            "Required-by: accelerate, fastai, peft, sentence-transformers, timm, torchaudio, torchdata, torchvision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINE ROCAUC optimizer\n",
        "\n",
        "_DESCRIPTION = \"\"\"\n",
        "This metric computes the area under the curve (AUC) for the Receiver Operating Characteristic Curve (ROC). The return values represent how well the model used is predicting the correct classes, based on the input data. A score of `0.5` means that the model is predicting exactly at chance, i.e. the model's predictions are correct at the same rate as if the predictions were being decided by the flip of a fair coin or the roll of a fair die. A score above `0.5` indicates that the model is doing better than chance, while a score below `0.5` indicates that the model is doing worse than chance.\n",
        "This metric has three separate use cases:\n",
        "    - binary: The case in which there are only two different label classes, and each example gets only one label. This is the default implementation.\n",
        "    - multiclass: The case in which there can be more than two different label classes, but each example still gets only one label.\n",
        "    - multilabel: The case in which there can be more than two different label classes, and each example can have more than one label.\n",
        "\"\"\"\n",
        "\n",
        "_KWARGS_DESCRIPTION = \"\"\"\n",
        "Args:\n",
        "- references (array-like of shape (n_samples,) or (n_samples, n_classes)): Ground truth labels. Expects different input based on use case:\n",
        "    - binary: expects an array-like of shape (n_samples,)\n",
        "    - multiclass: expects an array-like of shape (n_samples,)\n",
        "    - multilabel: expects an array-like of shape (n_samples, n_classes)\n",
        "- prediction_scores (array-like of shape (n_samples,) or (n_samples, n_classes)): Model predictions. Expects different inputs based on use case:\n",
        "    - binary: expects an array-like of shape (n_samples,)\n",
        "    - multiclass: expects an array-like of shape (n_samples, n_classes)\n",
        "    - multilabel: expects an array-like of shape (n_samples, n_classes)\n",
        "- average (`str`): Type of average, and is ignored in the binary use case. Defaults to 'macro'. Options are:\n",
        "    - `'micro'`: Calculates metrics globally by considering each element of the label indicator matrix as a label. Only works with the multilabel use case.\n",
        "    - `'macro'`: Calculate metrics for each label, and find their unweighted mean.  This does not take label imbalance into account.\n",
        "    - `'weighted'`: Calculate metrics for each label, and find their average, weighted by support (i.e. the number of true instances for each label).\n",
        "    - `'samples'`: Calculate metrics for each instance, and find their average. Only works with the multilabel use case.\n",
        "    - `None`:  No average is calculated, and scores for each class are returned. Only works with the multilabels use case.\n",
        "- sample_weight (array-like of shape (n_samples,)): Sample weights. Defaults to None.\n",
        "- max_fpr (`float`): If not None, the standardized partial AUC over the range [0, `max_fpr`] is returned. Must be greater than `0` and less than or equal to `1`. Defaults to `None`. Note: For the multiclass use case, `max_fpr` should be either `None` or `1.0` as ROC AUC partial computation is not currently supported for `multiclass`.\n",
        "- multi_class (`str`): Only used for multiclass targets, where it is required. Determines the type of configuration to use. Options are:\n",
        "    - `'ovr'`: Stands for One-vs-rest. Computes the AUC of each class against the rest. This treats the multiclass case in the same way as the multilabel case. Sensitive to class imbalance even when `average == 'macro'`, because class imbalance affects the composition of each of the 'rest' groupings.\n",
        "    - `'ovo'`: Stands for One-vs-one. Computes the average AUC of all possible pairwise combinations of classes. Insensitive to class imbalance when `average == 'macro'`.\n",
        "- labels (array-like of shape (n_classes,)): Only used for multiclass targets. List of labels that index the classes in\n",
        "    `prediction_scores`. If `None`, the numerical or lexicographical order of the labels in\n",
        "    `prediction_scores` is used. Defaults to `None`.\n",
        "Returns:\n",
        "    roc_auc (`float` or array-like of shape (n_classes,)): Returns array if in multilabel use case and `average='None'`. Otherwise, returns `float`.\n",
        "Examples:\n",
        "    Example 1:\n",
        "        >>> roc_auc_score = evaluate.load(\"roc_auc\")\n",
        "        >>> refs = [1, 0, 1, 1, 0, 0]\n",
        "        >>> pred_scores = [0.5, 0.2, 0.99, 0.3, 0.1, 0.7]\n",
        "        >>> results = roc_auc_score.compute(references=refs, prediction_scores=pred_scores)\n",
        "        >>> print(round(results['roc_auc'], 2))\n",
        "        0.78\n",
        "    Example 2:\n",
        "        >>> roc_auc_score = evaluate.load(\"roc_auc\", \"multiclass\")\n",
        "        >>> refs = [1, 0, 1, 2, 2, 0]\n",
        "        >>> pred_scores = [[0.3, 0.5, 0.2],\n",
        "        ...                 [0.7, 0.2, 0.1],\n",
        "        ...                 [0.005, 0.99, 0.005],\n",
        "        ...                 [0.2, 0.3, 0.5],\n",
        "        ...                 [0.1, 0.1, 0.8],\n",
        "        ...                 [0.1, 0.7, 0.2]]\n",
        "        >>> results = roc_auc_score.compute(references=refs, prediction_scores=pred_scores, multi_class='ovr')\n",
        "        >>> print(round(results['roc_auc'], 2))\n",
        "        0.85\n",
        "    Example 3:\n",
        "        >>> roc_auc_score = evaluate.load(\"roc_auc\", \"multilabel\")\n",
        "        >>> refs = [[1, 1, 0],\n",
        "        ...         [1, 1, 0],\n",
        "        ...         [0, 1, 0],\n",
        "        ...         [0, 0, 1],\n",
        "        ...         [0, 1, 1],\n",
        "        ...         [1, 0, 1]]\n",
        "        >>> pred_scores = [[0.3, 0.5, 0.2],\n",
        "        ...                 [0.7, 0.2, 0.1],\n",
        "        ...                 [0.005, 0.99, 0.005],\n",
        "        ...                 [0.2, 0.3, 0.5],\n",
        "        ...                 [0.1, 0.1, 0.8],\n",
        "        ...                 [0.1, 0.7, 0.2]]\n",
        "        >>> results = roc_auc_score.compute(references=refs, prediction_scores=pred_scores, average=None)\n",
        "        >>> print([round(res, 2) for res in results['roc_auc']])\n",
        "        [0.83, 0.38, 0.94]\n",
        "\"\"\"\n",
        "\n",
        "_CITATION = \"\"\"\\\n",
        "@article{doi:10.1177/0272989X8900900307,\n",
        "author = {Donna Katzman McClish},\n",
        "title ={Analyzing a Portion of the ROC Curve},\n",
        "journal = {Medical Decision Making},\n",
        "volume = {9},\n",
        "number = {3},\n",
        "pages = {190-195},\n",
        "year = {1989},\n",
        "doi = {10.1177/0272989X8900900307},\n",
        "    note ={PMID: 2668680},\n",
        "URL = {https://doi.org/10.1177/0272989X8900900307},\n",
        "eprint = {https://doi.org/10.1177/0272989X8900900307}\n",
        "}\n",
        "@article{10.1023/A:1010920819831,\n",
        "author = {Hand, David J. and Till, Robert J.},\n",
        "title = {A Simple Generalisation of the Area Under the ROC Curve for Multiple Class Classification Problems},\n",
        "year = {2001},\n",
        "issue_date = {November 2001},\n",
        "publisher = {Kluwer Academic Publishers},\n",
        "address = {USA},\n",
        "volume = {45},\n",
        "number = {2},\n",
        "issn = {0885-6125},\n",
        "url = {https://doi.org/10.1023/A:1010920819831},\n",
        "doi = {10.1023/A:1010920819831},\n",
        "journal = {Mach. Learn.},\n",
        "month = {oct},\n",
        "pages = {171–186},\n",
        "numpages = {16},\n",
        "keywords = {Gini index, AUC, error rate, ROC curve, receiver operating characteristic}\n",
        "}\n",
        "@article{scikit-learn,\n",
        "title={Scikit-learn: Machine Learning in {P}ython},\n",
        "author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},\n",
        "journal={Journal of Machine Learning Research},\n",
        "volume={12},\n",
        "pages={2825--2830},\n",
        "year={2011}\n",
        "}\n",
        "\"\"\"\n",
        "@evaluate.utils.file_utils.add_start_docstrings(_DESCRIPTION, _KWARGS_DESCRIPTION)\n",
        "class ROCAUC(evaluate.Metric):\n",
        "    def _info(self):\n",
        "        return evaluate.MetricInfo(\n",
        "            description=_DESCRIPTION,\n",
        "            citation=_CITATION,\n",
        "            inputs_description=_KWARGS_DESCRIPTION,\n",
        "            features=datasets.Features(\n",
        "                {\n",
        "                    \"prediction_scores\": datasets.Sequence(datasets.Value(\"float\")),\n",
        "                    \"references\": datasets.Value(\"int32\"),\n",
        "                }\n",
        "                if self.config_name == \"multiclass\"\n",
        "                else {\n",
        "                    \"references\": datasets.Sequence(datasets.Value(\"int32\")),\n",
        "                    \"prediction_scores\": datasets.Sequence(datasets.Value(\"float\")),\n",
        "                }\n",
        "                if self.config_name == \"multilabel\"\n",
        "                else {\n",
        "                    \"references\": datasets.Value(\"int32\"),\n",
        "                    \"prediction_scores\": datasets.Value(\"float\"),\n",
        "                }\n",
        "            ),\n",
        "            reference_urls=[\"https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html\"],\n",
        "        )\n",
        "\n",
        "    def _compute(\n",
        "        self,\n",
        "        references,\n",
        "        prediction_scores,\n",
        "        average=\"macro\",\n",
        "        sample_weight=None,\n",
        "        max_fpr=None,\n",
        "        multi_class=\"raise\",\n",
        "        labels=None,\n",
        "    ):\n",
        "        return {\n",
        "            \"roc_auc\": roc_auc_score(\n",
        "                references,\n",
        "                prediction_scores,\n",
        "                average=average,\n",
        "                sample_weight=sample_weight,\n",
        "                max_fpr=max_fpr,\n",
        "                multi_class=multi_class,\n",
        "                labels=labels,\n",
        "            )\n",
        "        }"
      ],
      "metadata": {
        "id": "NlPah7AKeKdT"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/tasinfrancesco/pml_challenge/main/train.csv\n",
        "!wget https://raw.githubusercontent.com/tasinfrancesco/pml_challenge/main/test.csv\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABbb0UQj8Avf",
        "outputId": "9bb1d2f0-d714-4b71-b959-710f5186e2cc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2026-01-19 13:13:46--  https://raw.githubusercontent.com/tasinfrancesco/pml_challenge/main/train.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1054962 (1.0M) [text/plain]\n",
            "Saving to: ‘train.csv’\n",
            "\n",
            "train.csv           100%[===================>]   1.01M  --.-KB/s    in 0.05s   \n",
            "\n",
            "2026-01-19 13:13:46 (20.9 MB/s) - ‘train.csv’ saved [1054962/1054962]\n",
            "\n",
            "--2026-01-19 13:13:46--  https://raw.githubusercontent.com/tasinfrancesco/pml_challenge/main/test.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 3243869 (3.1M) [text/plain]\n",
            "Saving to: ‘test.csv’\n",
            "\n",
            "test.csv            100%[===================>]   3.09M  --.-KB/s    in 0.09s   \n",
            "\n",
            "2026-01-19 13:13:46 (36.1 MB/s) - ‘test.csv’ saved [3243869/3243869]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### long version of the same code ###\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lanwuwei/BERTOverflow_stackoverflow_github\", padding = True)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"lanwuwei/BERTOverflow_stackoverflow_github\", num_labels = 2, problem_type = \"single_label_classification\")\n",
        "batch_size = 16\n",
        "num_epochs = 3\n",
        "optimizer = AdamW(model.parameters(), lr = 5e-5, weight_decay = .01)\n",
        "\n",
        "data_files = {\"train\": \"train.csv\", \"test\": \"test.csv\"}\n",
        "loaded_ds = load_dataset(\"csv\", data_files = data_files)\n",
        "print(f\"shape of 'tokenized_train': {loaded_ds[\"train\"].shape}\")\n",
        "print(f\"shape of 'tokenized_test': {loaded_ds[\"test\"].shape}\")\n",
        "\n",
        "small_train_val_ds = DatasetDict(\n",
        "    train = loaded_ds[\"train\"].shuffle(seed=42).select(range(128)),\n",
        "    val = loaded_ds[\"train\"].shuffle(seed=42).select(range(128, 160)),\n",
        ")\n",
        "\n",
        "\n",
        "small_tokenized_ds = small_train_val_ds.map(lambda example : tokenizer(example[\"text\"], padding = True),\n",
        "                                            batched = True,\n",
        "                                            batch_size = batch_size,\n",
        "                                            )\n",
        "\n",
        "\n",
        "small_tokenized_ds = small_tokenized_ds.remove_columns([\"text\"])\n",
        "small_tokenized_ds = small_tokenized_ds.rename_column(\"target\", \"labels\")\n",
        "small_tokenized_ds.set_format(\"torch\")\n",
        "\n",
        "train_dataloader = DataLoader(small_tokenized_ds[\"train\"], batch_size = batch_size)\n",
        "eval_dataloader = DataLoader(small_tokenized_ds[\"val\"], batch_size = batch_size)\n",
        "\n",
        "\n",
        "num_training_steps = 3*len(train_dataloader)\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps = num_training_steps)\n",
        "\n",
        "best_val_loss = float(\"inf\")\n",
        "progress_bar = tqdm(range(num_training_steps))\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  model.train()\n",
        "  for batch_i, batch in enumerate(train_dataloader):\n",
        "    output = model(input_ids = batch[\"input_ids\"],\n",
        "                     attention_mask = batch[\"attention_mask\"],\n",
        "                     token_type_ids = batch[\"token_type_ids\"],\n",
        "                     labels = batch[\"labels\"]\n",
        "                   )\n",
        "    optimizer.zero_grad()\n",
        "    output.loss.backward()\n",
        "    optimizer.step()\n",
        "    lr_scheduler.step()\n",
        "    progress_bar.update(1)\n",
        "  model.eval()\n",
        "  loss = 0\n",
        "  for batch_i, batch in enumerate(eval_dataloader):\n",
        "    with torch.no_grad():\n",
        "      output = model(input_ids = batch[\"input_ids\"],\n",
        "                     attention_mask = batch[\"attention_mask\"],\n",
        "                     token_type_ids = batch[\"token_type_ids\"],\n",
        "                     labels = batch[\"labels\"]\n",
        "                     )\n",
        "    loss += output.loss\n",
        "  avg_val_loss = loss/len(eval_dataloader)\n",
        "  print(f\"\\naverage validation loss = {avg_val_loss}\")\n",
        "  if avg_val_loss<best_val_loss:\n",
        "    torch.save({\n",
        "        \"epoch\" : epoch,\n",
        "        \"model_state_dict\" : model.state_dict(),\n",
        "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "        \"val_loss\": best_val_loss},\n",
        "               f\"epoch_{epoch}.pt\"\n",
        "               )\n"
      ],
      "metadata": {
        "id": "_966ggApXdiA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "3234422bf7b84c209ecd580012d62b1c",
            "5a411ca8c52d477499b065029f32a983",
            "6696b984904f41eca3a6dd246655f80d",
            "7cd861dada1947e1b0b666c090fc97a5",
            "80160ec329bb445f8d325aecb69a595d",
            "0609a23a40694d9a891e6115998a9181",
            "d667f53ba8c84df89b52f2a46f178ab2",
            "a2755662f50f4e4892499dde78d81760",
            "191bc4ea0c8f4468879761c47857df32",
            "37890620ac764cdfbc081322158a0905",
            "799ecb12b537415ea1a2370801bb45d0"
          ]
        },
        "outputId": "5c4af0ee-0339-4233-c2da-7e0b045cd0fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at lanwuwei/BERTOverflow_stackoverflow_github and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "shape of 'tokenized_train': (5000, 7)\n",
            "shape of 'tokenized_test': (15000, 7)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/24 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3234422bf7b84c209ecd580012d62b1c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "average validation loss = 0.40866944193840027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "help(model.forward(train_dataloader))"
      ],
      "metadata": {
        "id": "uuzwg4XPj6Cl",
        "outputId": "61a27067-7a5b-460d-9b04-ff8bee2fc67d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        }
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "'DataLoader' object is not subscriptable",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-832133400.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: 'DataLoader' object is not subscriptable"
          ]
        }
      ]
    }
  ]
}