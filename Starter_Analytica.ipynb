{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasinfrancesco/Practical_ML_PSL/blob/main/Starter_Analytica.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076d91a2",
      "metadata": {
        "id": "076d91a2"
      },
      "source": [
        "# NB8 - Reproducing Cambridge Analytica's work\n",
        "\n",
        "Cambridge Analytica was a London-based company founded in 2013, specializing in Big Data analysis and the application of psychological profiling. One of their most well-known methods was based on the Big Five personality traits, also known as the OCEAN model. The OCEAN model suggests that human personality can be described using five broad dimensions:\n",
        "\n",
        "* **Extraversion** : Reflects a person's tendency to seek stimulation in the company of others. High scorers are outgoing, talkative, and energetic, while low scorers tend to be reserved and prefer solitary activities.\n",
        "* **Neuroticism** : Measures emotional stability and the tendency to experience negative emotions like anxiety, sadness, or irritability. Higher scores indicate greater emotional sensitivity, while lower scores suggest calm and resilience under stress.\n",
        "* **Agreeableness** : Captures the extent to which a person is cooperative, empathetic, and willing to help others. High scorers are compassionate and trusting, while low scorers may be more competitive or skeptical of others' motives.\n",
        "* **Conscientiousness** : Describes a person’s level of self-discipline, organization, and goal-directed behavior. High scorers are diligent, detail-oriented, and reliable, while low scorers may be more spontaneous or less focused on long-term goals.\n",
        "* **Openness** : Reflects a person's openness to new experiences, creativity, and curiosity. High scorers are imaginative and adventurous, while low scorers prefer routine and are more traditional in their views.\n",
        "\n",
        "In addition to psychological profiling, Cambridge Analytica’s services were employed by various political campaigns globally, most notably by candidates such as Ben Carson, Ted Cruz, and Donald Trump during the 2016 U.S. presidential election. The company faced significant controversy for allegedly harvesting personal data from millions of Facebook users. They used a quiz, which users voluntarily completed, to gather information about individuals' personalities. This data, combined with users' social connections, posts, and likes, enabled Cambridge Analytica to target potential voters for political campaigns, influencing their strategies.\n",
        "\n",
        "In this notebook, we aim to replicate a simplified version of this work, but without the ethical concerns. Instead of illicitly collecting data, we use publicly available responses to the OCEAN personality test. As outlined in the supplementary Python files, we generated a target column to simulate identifying individuals who might be inclined to vote for a particular political candidate. The goal is to explore how personality traits could potentially correlate with political preferences in a machine learning context.\n",
        "\n",
        "This is not 100% accurate, but the main ideas are the ones that were used by Cambridge Analytica. The steps shown here are not always the most efficient or the most \"industry-approved.\" Their main purpose is pedagogical. So don't panic if something looks suboptimal—it's meant to be.\n",
        "\n",
        "If you have questions (theoretical or practical), don't hesitate to bug your lecturer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe699377",
      "metadata": {
        "id": "fe699377"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.random.seed(42)\n",
        "\n",
        "df = pd.read_csv(\"Analytica.csv\")\n",
        "\n",
        "OCEAN = {\n",
        "    \"EXT\": \"Extraversion\",\n",
        "    \"EST\": \"Neuroticism\",\n",
        "    \"AGR\": \"Agreeableness\",\n",
        "    \"CSN\": \"Conscientiousness\",\n",
        "    \"OPN\": \"Openness\"\n",
        "}\n",
        "dimensions = list(OCEAN.keys())\n",
        "\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "404a2818",
      "metadata": {
        "id": "404a2818"
      },
      "source": [
        "## Analysis\n",
        "\n",
        "We have 10 question per dimension, and the time in second taken to respond to each question. We also have the overall score for each dimension and the time take to answer all of the questions.\n",
        "Let's first have a look at the distribution for the overall scores.\n",
        "\n",
        "**Task** : Using the code in the following cell, analyse its output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1dc1acd5",
      "metadata": {
        "id": "1dc1acd5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns; sns.set_style(style=\"whitegrid\")\n",
        "\n",
        "plt.figure(figsize=(15, 8))\n",
        "\n",
        "for index in range(1, 6):\n",
        "    plt.subplot(2, 5, index)\n",
        "    column = dimensions[index-1]\n",
        "    plt.hist(df[column])\n",
        "    plt.xlim(-1, 1)\n",
        "    plt.ylim(0, 8500)\n",
        "    plt.title(column)\n",
        "\n",
        "    plt.subplot(2, 5, index+5)\n",
        "    column = column + \"_time\"\n",
        "    plt.hist(df[column])\n",
        "    plt.xlim(0, 250)\n",
        "    plt.ylim(0, 16500)\n",
        "    plt.title(column)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2044d565",
      "metadata": {
        "id": "2044d565"
      },
      "source": [
        "Before diving in the modelisation, one need to check for missing values.\n",
        "\n",
        "**Task** : Using the [`isna`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.isna.html) method along with the [`sum`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.sum.html#pandas.DataFrame.sum) method, look for missing values in the dataset. As there is 100+ columns, only display the ones with missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1db662c5",
      "metadata": {
        "id": "1db662c5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "2ee04e5d",
      "metadata": {
        "id": "2ee04e5d"
      },
      "source": [
        "As expected, we have only 15% *target* variable available. Our goal is to regroup people that *looks* the same and then project the target into the groups.\n",
        "\n",
        "## Modelisation\n",
        "\n",
        "But first, as we saw, we need to scale all the distributions.\n",
        "\n",
        "**Task** : Extract from the dataframe all the columns except the target one, then use the [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) class. Keep the target in a vector named *y* for convenience."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7414d088",
      "metadata": {
        "id": "7414d088"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1c962e07",
      "metadata": {
        "id": "1c962e07"
      },
      "source": [
        "As we have 112 columns, we should first reduce the dimension of our dataset.\n",
        "\n",
        "### PCA\n",
        "\n",
        "We will use [`PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) (Principal Component Analysis) to do this.\n",
        "\n",
        "**Task** : Reduce the dimension to 20 variables using the `fit` then `transform` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "71f94904",
      "metadata": {
        "id": "71f94904"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "f90a34ce",
      "metadata": {
        "id": "f90a34ce"
      },
      "source": [
        "Can we measure how *good* our PCA is ? The theory tells that the sum of all the eigenvalue of dimension is equal to the overall variance of the dataset. Here we kept only 20 over 112, so how much of variance did we *explained* ?\n",
        "\n",
        "**Task** : Using the `explained_variance_ratio` attribute in the PCA model, answer the question."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b02030ed",
      "metadata": {
        "id": "b02030ed"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "083c2b09",
      "metadata": {
        "id": "083c2b09"
      },
      "source": [
        "So we kept ~50% of the variance using only 18% dimensions (20 / 111) which is good but not great. We will try to improve this part later.\n",
        "\n",
        "### KMeans\n",
        "\n",
        "As we have a smaller dataset in terms of dimension, the KMeans algorithm can be used.\n",
        "\n",
        "**Task** : Using the [`KMeans`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html) class, get a vector named *cluster* which is the result of the `predict` method of [`KMeans`] class. We will build 5 clusters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28b45508",
      "metadata": {
        "id": "28b45508"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "1b9b2c8b",
      "metadata": {
        "id": "1b9b2c8b"
      },
      "source": [
        "And... is it good ? We do not have any ground truth this time, but we can look at the [`silhouette score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.silhouette_score.html) function to have an idea of how well cluster are shaped.\n",
        "\n",
        "**Task** : Compute and analyse the silhouette score of our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a8ed8cbd",
      "metadata": {
        "id": "a8ed8cbd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6e4f5ce6",
      "metadata": {
        "id": "6e4f5ce6"
      },
      "source": [
        "It is not great... We will improve this value but we need to *understand* what the clusters are about.\n",
        "\n",
        "### Analysis\n",
        "\n",
        "**Task** : Extract for the original dataframe the 5 overall score and the target in a dataframe, then add the predicted cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68e1ceeb",
      "metadata": {
        "id": "68e1ceeb"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e264e4da",
      "metadata": {
        "id": "e264e4da"
      },
      "source": [
        "We are interested in the *target* column, which is binary. Therefore we would like to know the distribution in each cluster for the target.\n",
        "\n",
        "**Task** : Use the [`groupby`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.groupby.html) method along with the [`agg`](https://pandas.pydata.org/docs/reference/api/pandas.core.groupby.DataFrameGroupBy.agg.html) method to get both the number in each cluster (using *count*) and the distribution (using *mean*). Then analyse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5cf4385f",
      "metadata": {
        "id": "5cf4385f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4b2d6742",
      "metadata": {
        "id": "4b2d6742"
      },
      "source": [
        "To better grasp the distribution of the each cluster according to the overall score, we will use the [`barplot`](https://seaborn.pydata.org/generated/seaborn.barplot.html#seaborn.barplot) function of seaborn."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ca342ba",
      "metadata": {
        "id": "0ca342ba"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 4))\n",
        "\n",
        "for index, dimension in enumerate(dimensions):\n",
        "    plt.subplot(1, 5, index+1)\n",
        "    sns.barplot(data=study, x=cluster, y=dimension, estimator=\"mean\")\n",
        "    plt.xlabel(\"Cluster\")\n",
        "    plt.ylabel(\"\")\n",
        "    plt.ylim(-1, 1)\n",
        "    plt.title(dimension)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c53e7e0a",
      "metadata": {
        "id": "c53e7e0a"
      },
      "source": [
        "We need to do a bit better.\n",
        "\n",
        "## Time to improve\n",
        "\n",
        "We have three areas to improve :\n",
        "1. Feature selections : is it necessary to keep all the variables ?\n",
        "2. PCA components : what is the optimal value for the number of components to keep ?\n",
        "3. KMeans cluster : what is the optimal value for the number of clusters ?\n",
        "\n",
        "We will focus only on the last two questions here, and left the first to the reader. Let's start with the PCA, as it will impact the clustering.\n",
        "\n",
        "### PCA Components\n",
        "\n",
        "We have a measure of explained variance, we shall use it. Before we need to *refresh* our dataset so that we do not keep previous transformations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "587661fd",
      "metadata": {
        "id": "587661fd"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"Analytica.csv\")\n",
        "X = df.drop(columns=[\"target\"])\n",
        "y = df[\"target\"]\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(X)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "863060c6",
      "metadata": {
        "id": "863060c6"
      },
      "source": [
        "**Task** : Compute the explained variance for different values of components to keep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2d78a5d6",
      "metadata": {
        "id": "2d78a5d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "8babb36c",
      "metadata": {
        "id": "8babb36c"
      },
      "source": [
        "**Task** : Display the result of the previous cell into a self explanatory plot. Use it to make a decision on the optimal number of components to keep.\n",
        "\n",
        "Also bear in mind that the more components are kept, the less meaning distance will have for the KMeans clustering."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13ad82d6",
      "metadata": {
        "id": "13ad82d6"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5a6085e4",
      "metadata": {
        "id": "5a6085e4"
      },
      "source": [
        "**Task** : Given your previous analysis, use PCA to reduce dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d09799e4",
      "metadata": {
        "id": "d09799e4"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "52fe707c",
      "metadata": {
        "id": "52fe707c"
      },
      "source": [
        "### Number of clusters\n",
        "\n",
        "Now that we have an *optimal* feature matrix, we need to find the better number of cluster for the KMeans algorithm. The model, once trained, as an attribute `inertia` that can be tracked to get insights on how splitted the clusters are.\n",
        "\n",
        "**Task** : Compute the inertia for several number of cluster, up to 30 for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30dfb100",
      "metadata": {
        "id": "30dfb100"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "fd941a23",
      "metadata": {
        "id": "fd941a23"
      },
      "source": [
        "**Task** : Display the informations of the previous cell into a self-explanatory plot. Use it to make a decision on the optimal number of cluster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c52f8bd3",
      "metadata": {
        "id": "c52f8bd3"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "90f458f5",
      "metadata": {
        "id": "90f458f5"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "19a9bed1",
      "metadata": {
        "id": "19a9bed1"
      },
      "source": [
        "**Task** : Adapt the following cells (if needed) given your previous choice, and the choices above.\n",
        "\n",
        "We are creating a dataframe with two columns :\n",
        "* **ID** : the identifier of the person\n",
        "* **prediction** : your prediction on if he is going to vote for your candidate or not (0 / 1)\n",
        "The dataframe must only contains the rows with no target known. The prediction will come from the average target in each cluster we build. We will predict 1 if the average is above 0.5, 0 instead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a67aac8a",
      "metadata": {
        "id": "a67aac8a"
      },
      "outputs": [],
      "source": [
        "study = df[[\"ID\", \"target\"]]\n",
        "study[\"Cluster\"] = cluster\n",
        "clusters = study.groupby(by=\"Cluster\", as_index=False).agg(\"mean\")[[\"Cluster\", \"target\"]]\n",
        "clusters.columns = [\"Cluster\", \"Average\"]\n",
        "\n",
        "study = study.merge(clusters, on=\"Cluster\")\n",
        "study = study.loc[study[\"target\"].isna(), ]\n",
        "study[\"prediction\"] = (study[\"Average\"] > 0.5).astype(int)\n",
        "answer = study[[\"ID\", \"prediction\"]]\n",
        "answer.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f72094fc",
      "metadata": {
        "id": "f72094fc"
      },
      "source": [
        "**Task** : save the dataframe *answer* and submit your prediction on the [Kaggle competition](https://www.kaggle.com/t/4817e6d424c040a98571c0277e88b18a)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6e60a6a",
      "metadata": {
        "id": "a6e60a6a"
      },
      "outputs": [],
      "source": [
        "answer.to_csv(\"submit.csv\", index=False)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}