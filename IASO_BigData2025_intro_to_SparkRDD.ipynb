{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasinfrancesco/Practical_ML_PSL/blob/main/IASO_BigData2025_intro_to_SparkRDD.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbSYrBvCMGKi",
        "outputId": "87beb436-6b28-4dd3-d48f-695ca98f1817"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: Skipping google-cloud-dataproc-spark-connect as it is not installed.\u001b[0m\u001b[33m\n",
            "\u001b[0mW: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.5/200.5 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Spark version: 3.5.3\n",
            "openjdk version \"17.0.17\" 2025-10-21\n",
            "[(1, 1), (2, 4), (3, 9), (4, 16), (5, 25)]\n"
          ]
        }
      ],
      "source": [
        "# =========================\n",
        "# Colab: PySpark classic (RDD-friendly) + remove Spark Connect conflict\n",
        "# =========================\n",
        "\n",
        "# 0) Remove the package that forces pyspark[connect] ~= 4.0.0\n",
        "!pip -q uninstall -y dataproc-spark-connect google-cloud-dataproc-spark-connect || true  # package name can vary\n",
        "\n",
        "# 1) Install Java + pinned PySpark (classic)\n",
        "!apt-get update -qq\n",
        "!apt-get install -y openjdk-17-jdk-headless -qq\n",
        "!pip -q install pyspark==3.5.3\n",
        "\n",
        "# 2) Configure env + start Spark\n",
        "import os, subprocess\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
        "os.environ[\"PATH\"] = os.environ[\"JAVA_HOME\"] + \"/bin:\" + os.environ[\"PATH\"]\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"colab-rdd\")\n",
        "         .config(\"spark.ui.showConsoleProgress\", \"false\")\n",
        "         .getOrCreate())\n",
        "\n",
        "sc = spark.sparkContext\n",
        "print(\"Spark version:\", spark.version)\n",
        "print(subprocess.check_output([\"java\",\"-version\"], stderr=subprocess.STDOUT).decode().splitlines()[0])\n",
        "\n",
        "# 3) Quick RDD check\n",
        "rdd = sc.parallelize([1,2,3,4,5], 2).map(lambda x: (x, x*x))\n",
        "print(rdd.collect())"
      ]
    }
  ]
}