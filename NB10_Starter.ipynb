{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasinfrancesco/Practical_ML_PSL/blob/main/NB10_Starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c31512bf",
      "metadata": {
        "id": "c31512bf"
      },
      "source": [
        "# Lab Work 10 : NLP Basics\n",
        "\n",
        "This notebook builds on the tenth lecture of Foundations of Machine Learning. We'll focus on some *traditionnal* NLP technics, meaning not using any *transformer* architectures.\n",
        "\n",
        "Important note: the steps shown here are not always the most efficient or the most \"industry-approved.\" Their main purpose is pedagogical. So don't panic if something looks suboptimalâ€”it's meant to be.\n",
        "\n",
        "If you have questions (theoretical or practical), don't hesitate to bug your lecturer.\n",
        "\n",
        "We will try to accurately predict if a tweet has been written by Donald Trump (until its account was banned) or by an AI. To build this dataset, we used a dataset that has collected several Donald Trump's tweet and we manually ask several models to wrote copies. More details on how the dataset was made in a separate notebook.\n",
        "Let's load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "90e7fb81",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "90e7fb81",
        "outputId": "7af3f7c3-2743-42e3-9ed3-fb3ccae56179"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'Donald_or_AI_train.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2826559263.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Donald_or_AI_train.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Donald_or_AI_train.csv'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"Donald_or_AI_train.csv\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a7ed05dd",
      "metadata": {
        "id": "a7ed05dd"
      },
      "source": [
        "Tweets can be a challenge for NLP techniques :\n",
        "* Only short snippets of text : we kept only tweets below 150 characters\n",
        "* Some Twitter/X specificity : the \"@\" character and the \"#\" can carry meanings\n",
        "* There can be some emojis in it\n",
        "\n",
        "We will focus first on some cleaning before diving in the modelling.\n",
        "\n",
        "## Data cleaning\n",
        "\n",
        "In order to know what to perform, we suggest looking at some tweets or fake tweets first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "5f41a354",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "5f41a354",
        "outputId": "0408ec43-644d-4339-9385-f86b190cb749"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2674770829.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mrandom\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"writer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"Original\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mstring\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mindexes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "from random import sample\n",
        "\n",
        "df[\"writer\"] = df[\"model\"].apply(lambda string: \"Original\" if pd.isna(string) else string)\n",
        "\n",
        "indexes = sample(range(df.shape[0]), k=10)\n",
        "for index in indexes:\n",
        "    tweet = df[\"content\"][index]\n",
        "    writer = df[\"writer\"][index]\n",
        "    print(f\"[{writer}] {tweet}\")\n",
        "    print(\"-\"*25)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e8ef74d",
      "metadata": {
        "id": "5e8ef74d"
      },
      "source": [
        "Given several rolls, we can see some points to clean :\n",
        "* Some tweets ends with \"\\n\" caracter\n",
        "* Some tweets are all within double quotes\n",
        "* Some tweets starts with \". \" then a quote, it is unnecessary to keep\n",
        "* Some tweets have double spaces, it is unnecessary to keep\n",
        "\n",
        "Beside this format, we also note that the deepseek-r1 model wrote very long tweet. Let's display one :"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e52358f6",
      "metadata": {
        "id": "e52358f6"
      },
      "outputs": [],
      "source": [
        "print(df.loc[df[\"model\"] == \"deepseek-r1:1.5b\", ][\"content\"].values[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9f0c867",
      "metadata": {
        "id": "e9f0c867"
      },
      "source": [
        "The Deepseek-R1 model is a **reasoning model** meaning he *thinks* before answering. The only part we need here is the part outside of the thinking process.\n",
        "\n",
        "**Task** : Given all the previous discussion, write a `clean_tweet` function. It will also lower all characters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d71c4f0e",
      "metadata": {
        "id": "d71c4f0e"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0c3e604f",
      "metadata": {
        "id": "0c3e604f"
      },
      "source": [
        "Now that we have *cleaner* tweets to work on, we need to build features from it.\n",
        "\n",
        "## Exploration and feature engineering\n",
        "\n",
        "**Task** : Create the following columns, with only the first one being build on the raw tweets. The rest of them shall be derived from a cleaned version.\n",
        "* `uppercase_ratio` : the proportion of uppercase character in the whole text. We may use the `isupper` method for a string.\n",
        "* `character_count` : the number of character in the text\n",
        "* `word_count` : the number of words in the text\n",
        "* `avg_word_length` : the average length of words in the tweet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cf32327",
      "metadata": {
        "id": "3cf32327"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "4e7edfd4",
      "metadata": {
        "id": "4e7edfd4"
      },
      "source": [
        "We would like to see if the previous indicators we build might already help identifying AI.\n",
        "\n",
        "**Task** : Using seaborn's [`histplot`](https://seaborn.pydata.org/generated/seaborn.histplot.html) function with the appropriate parameters, explore the columns. We shall use the *hue* parameter with either the target column `generated` or the `writer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0997bee7",
      "metadata": {
        "id": "0997bee7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "72135861",
      "metadata": {
        "id": "72135861"
      },
      "source": [
        "## First modelisation\n",
        "\n",
        "With not much work, can we already perform a classification ?\n",
        "\n",
        "**Task** : Train a [`LogisticRegression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) with a [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler) in a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html). We shall use the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function to measure performance. Here, we'll use the [`f1_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html) metric, so we will probably need the [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html) function.\n",
        "Bonus: use a [`TunedThresholdClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.TunedThresholdClassifierCV.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5e13c7f7",
      "metadata": {
        "id": "5e13c7f7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a317df96",
      "metadata": {
        "id": "a317df96"
      },
      "source": [
        "Already quite good performance without using the text *directly*.\n",
        "\n",
        "## NLP modelisation\n",
        "\n",
        "But it should be better with it.\n",
        "\n",
        "**Task** : Still using a [`Pipeline`](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) class and the [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) function, now use the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) class with english stopwords and display results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2a9c160",
      "metadata": {
        "id": "e2a9c160"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "7d1e66c5",
      "metadata": {
        "id": "7d1e66c5"
      },
      "source": [
        "That is better ! But we could imagine stronger performance if we tuned a bit the vectorizer.\n",
        "\n",
        "**Task** : Using the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class, find better parameter for the vectorizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "812019cd",
      "metadata": {
        "id": "812019cd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "e8ffd060",
      "metadata": {
        "id": "e8ffd060"
      },
      "source": [
        "But we only used words this time, not the previous statistics we had.\n",
        "\n",
        "## Third modelisation : combining approach\n",
        "\n",
        "**Task** : Using the [`ColumnTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.compose.ColumnTransformer.html#sklearn.compose.ColumnTransformer), rewrite the pipeline so that it uses both numeric and text features.\n",
        "\n",
        "As the [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) expect 1D array, one need to use first [`FunctionTransformer`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.FunctionTransformer.html#sklearn.preprocessing.FunctionTransformer) to flatten the input using the `squeeze` method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "541cb14f",
      "metadata": {
        "id": "541cb14f"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "5b44bf6c",
      "metadata": {
        "id": "5b44bf6c"
      },
      "source": [
        "It is better ! Time to fit it, then use the test set and submit on the [Kaggle competition](https://www.kaggle.com/t/db2bae0e9d814baa96a0468f021cd3f2).\n",
        "\n",
        "**Task** : Rewrite your feature engineering pipeline and use it to submit."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ce07e55b",
      "metadata": {
        "id": "ce07e55b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "a42e3e5a",
      "metadata": {
        "id": "a42e3e5a"
      },
      "source": [
        "Now, it is up to you to make the performance better ! Here are some guidelines:\n",
        "1. Make the dataset *cleaner* : there are probably still some work to do\n",
        "2. Build better features : more useful statistics can be extracted\n",
        "3. Find the most suitable model : note that we didn't fine-tune it, only the vectorizer..."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}