{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOTBTwI3UnVeqtmwXj4HH94",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a33eb10ec74b412d8b2d4e69c016afaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e0c2b383a55a44a0b649eccd6b4bfb62",
              "IPY_MODEL_af4573e9ba034627a2f088506a1138d9",
              "IPY_MODEL_2c1dd48c2e6d49898491e7d4370767a9"
            ],
            "layout": "IPY_MODEL_0c837315f12f49a382fc5c800728f85c"
          }
        },
        "e0c2b383a55a44a0b649eccd6b4bfb62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_57e3a003b75a423dbfac0b983995d656",
            "placeholder": "​",
            "style": "IPY_MODEL_ab47b701f1e34a418a8d1ab85ed81422",
            "value": "model.safetensors: 100%"
          }
        },
        "af4573e9ba034627a2f088506a1138d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76fedc6634aa452897a05e8fd2b4f56b",
            "max": 596091758,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e3ec03d8d36a40d9a64046d9423fabd8",
            "value": 596091758
          }
        },
        "2c1dd48c2e6d49898491e7d4370767a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d026e73714b34da6910c128efdbb2f1c",
            "placeholder": "​",
            "style": "IPY_MODEL_fba9fd00902c49bb9869c0247de5de70",
            "value": " 596M/596M [00:09&lt;00:00, 91.9MB/s]"
          }
        },
        "0c837315f12f49a382fc5c800728f85c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "57e3a003b75a423dbfac0b983995d656": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab47b701f1e34a418a8d1ab85ed81422": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76fedc6634aa452897a05e8fd2b4f56b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ec03d8d36a40d9a64046d9423fabd8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d026e73714b34da6910c128efdbb2f1c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fba9fd00902c49bb9869c0247de5de70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tasinfrancesco/Practical_ML_PSL/blob/main/ML_submission_Tasin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ds6qDsKCeG_W"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from time import strptime\n",
        "df = pd.read_csv(\"train.csv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.describe())\n",
        "print(df.head(5))\n",
        "# dt = pd.read_csv(\"test.csv\")\n",
        "# print(dt[[\"UTC\" not in cd for cd in dt[\"creation_date\"]]])\n",
        "type(df[\"creation_date\"][0])\n",
        "print(strptime(df[\"creation_date\"][0], \"%Y-%m-%d %H:%M:%S.%f %Z\"))\n",
        "\n",
        "print(strptime(df[\"creation_date\"][1], \"%Y-%m-%d %H:%M:%S.%f %Z\"))\n",
        "print(df[\"creation_date\"][0])\n",
        "print(df[\"creation_date\"][1])\n",
        "\n",
        "def timeinfo(timestr):\n",
        "  time_struct = strptime(df[\"creation_date\"][0], \"%Y-%m-%d %H:%M:%S.%f %Z\")\n",
        "\n",
        "  #is it a weekend\n",
        "  if time_struct.tm_wday >= 5:\n",
        "    weekend = 1\n",
        "  else:\n",
        "    weekend = 0\n",
        "\n",
        "  #is it a vacation\n",
        "\n"
      ],
      "metadata": {
        "id": "BqafgUcieSGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideas for feature engineering:\n",
        "\n",
        "  - local comment frequency (estimated from change in post_id over time window)\n",
        "  - is it a vacation\n",
        "  - is it a weekend\n",
        "\n",
        "post\n",
        "\n",
        "  - post length\n",
        "  - positivity of content?\n",
        "  - politeness?\n",
        "\n",
        "BERT:\n",
        "  - encode in some way politeness? (there's a stanford thingy)\n",
        "  - CodeBERT vs BERT?\n",
        "  - hide vectors of code/cut it out for text processing?\n",
        "  - clarity as:\n",
        "    - readability\n",
        "    - sentence/word length\n",
        "    - coding depth (nesting)\n",
        "    - number of questions\n",
        "    - presence of errors\n",
        "    - number of topics mentioned (\"I tried...\")\n",
        "\n",
        "User ID?\n",
        "\n",
        "  - is it better to predict for a given user (average) or for single posts?\n",
        "\n",
        "text vectorisation:\n",
        "\n",
        "analysis:\n",
        "\n",
        "  - logistic regression (not great given collinearity)\n",
        "  - random trees?\n",
        "\n",
        "predict score, then guess target (not target directly)\n"
      ],
      "metadata": {
        "id": "NW_2G-4HeXDZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install sentencepiece\n",
        "# !pip install protobuf==3.20.0\n",
        "from collections import defaultdict, Counter\n",
        "import json\n",
        "import transformers\n",
        "from transformers import AutoTokenizer\n",
        "from transformers import AutoModelForTokenClassification\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"lanwuwei/BERTOverflow_stackoverflow_github\")\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"lanwuwei/BERTOverflow_stackoverflow_github\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121,
          "referenced_widgets": [
            "a33eb10ec74b412d8b2d4e69c016afaf",
            "e0c2b383a55a44a0b649eccd6b4bfb62",
            "af4573e9ba034627a2f088506a1138d9",
            "2c1dd48c2e6d49898491e7d4370767a9",
            "0c837315f12f49a382fc5c800728f85c",
            "57e3a003b75a423dbfac0b983995d656",
            "ab47b701f1e34a418a8d1ab85ed81422",
            "76fedc6634aa452897a05e8fd2b4f56b",
            "e3ec03d8d36a40d9a64046d9423fabd8",
            "d026e73714b34da6910c128efdbb2f1c",
            "fba9fd00902c49bb9869c0247de5de70"
          ]
        },
        "id": "Hv9DzrqceUnl",
        "outputId": "0b6017b5-672d-4c29-f33a-d3f8b234b5de"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:torchao.kernel.intmm:Warning: Detected no triton, on systems without Triton certain kernels will not work\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/596M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a33eb10ec74b412d8b2d4e69c016afaf"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForTokenClassification were not initialized from the model checkpoint at lanwuwei/BERTOverflow_stackoverflow_github and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "outputs_1_to_10 = []\n",
        "cls = [tokenizer.cls_token_id]\n",
        "sep = [tokenizer.sep_token_id]\n",
        "for t in df[\"text\"][0:10]:\n",
        "  tok = tokenizer.tokenize(t)\n",
        "  ids = tokenizer.convert_tokens_to_ids(tok)\n",
        "  ids_special_tokens = cls + ids + sep\n",
        "\n",
        "  decoded_str = tokenizer.decode(ids_special_tokens)\n",
        "\n",
        "  print(\"start:               \", t)\n",
        "  print(\"tokenize:            \", tok)\n",
        "  print(\"convert_tokens_to_ids\", ids)\n",
        "  print(\"add special tokens:  \", ids_special_tokens)\n",
        "  print(\"================\")\n",
        "  print(\"decode:              \", decoded_str)\n",
        "  print(\"\\n\\n\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vA2LHWA83Dsr",
        "outputId": "81d890ed-8874-4447-807c-f5875c343808"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start:                Sincerely appreciate your taking a look. I never have to query across partitions. Disk space is not an issue. The volume of data definitely justifies the partitioning. Separate tables and or databases are not the option. Still my question remains unanswered :-( Many thanks in advance.\n",
            "tokenize:             ['Sincerely', 'appreciate', 'your', 'taking', 'a', 'look', '.', 'I', 'never', 'have', 'to', 'query', 'across', 'partitions', '.', 'Disk', 'space', 'is', 'not', 'an', 'issue', '.', 'The', 'volume', 'of', 'data', 'definitely', 'justifies', 'the', 'partitioning', '.', 'Separate', 'tables', 'and', 'or', 'databases', 'are', 'not', 'the', 'option', '.', 'Still', 'my', 'question', 'remains', 'unanswered', ':', '-', '(', 'Many', 'thanks', 'in', 'advance', '.']\n",
            "convert_tokens_to_ids [52297, 5429, 1881, 4670, 69, 2076, 18, 45, 3071, 1851, 1790, 2400, 4036, 12207, 18, 17326, 3257, 1804, 1859, 1794, 2456, 18, 1911, 7817, 1815, 1943, 5480, 57754, 1783, 14744, 18, 21624, 3133, 1809, 1876, 5494, 1894, 1859, 1783, 2685, 18, 8020, 1887, 2318, 6989, 23991, 30, 17, 12, 6341, 4453, 1797, 4152, 18]\n",
            "add special tokens:   [2, 52297, 5429, 1881, 4670, 69, 2076, 18, 45, 3071, 1851, 1790, 2400, 4036, 12207, 18, 17326, 3257, 1804, 1859, 1794, 2456, 18, 1911, 7817, 1815, 1943, 5480, 57754, 1783, 14744, 18, 21624, 3133, 1809, 1876, 5494, 1894, 1859, 1783, 2685, 18, 8020, 1887, 2318, 6989, 23991, 30, 17, 12, 6341, 4453, 1797, 4152, 18, 4]\n",
            "================\n",
            "decode:               [CLS] Sincerely appreciate your taking a look. I never have to query across partitions. Disk space is not an issue. The volume of data definitely justifies the partitioning. Separate tables and or databases are not the option. Still my question remains unanswered : - ( Many thanks in advance. [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                That's not what I'm doing right now ?\n",
            "The MapRoute provide the Markers to the MapController, but my question is more about how to handle interaction between the filters and the map.\n",
            "I'm going to take a look at the components, sounds good!\n",
            "tokenize:             ['That', \"'\", 's', 'not', 'what', 'I', \"'\", 'm', 'doing', 'right', 'now', '?', 'The', 'MapRoute', 'provide', 'the', 'Markers', 'to', 'the', 'Map', '##Controller', ',', 'but', 'my', 'question', 'is', 'more', 'about', 'how', 'to', 'handle', 'interaction', 'between', 'the', 'filters', 'and', 'the', 'map', '.', 'I', \"'\", 'm', 'going', 'to', 'take', 'a', 'look', 'at', 'the', 'components', ',', 'sounds', 'good', '!']\n",
            "convert_tokens_to_ids [2693, 11, 87, 1859, 2015, 45, 11, 81, 2503, 2424, 2389, 35, 1911, 50672, 3202, 1783, 33534, 1790, 1783, 5319, 3316, 16, 1868, 1887, 2318, 1804, 2122, 2201, 1995, 1790, 3043, 7219, 2569, 1783, 6520, 1809, 1783, 2901, 18, 45, 11, 81, 2774, 1790, 2725, 69, 2076, 1946, 1783, 4378, 16, 5125, 2565, 5]\n",
            "add special tokens:   [2, 2693, 11, 87, 1859, 2015, 45, 11, 81, 2503, 2424, 2389, 35, 1911, 50672, 3202, 1783, 33534, 1790, 1783, 5319, 3316, 16, 1868, 1887, 2318, 1804, 2122, 2201, 1995, 1790, 3043, 7219, 2569, 1783, 6520, 1809, 1783, 2901, 18, 45, 11, 81, 2774, 1790, 2725, 69, 2076, 1946, 1783, 4378, 16, 5125, 2565, 5, 4]\n",
            "================\n",
            "decode:               [CLS] That's not what I'm doing right now? The MapRoute provide the Markers to the MapController, but my question is more about how to handle interaction between the filters and the map. I'm going to take a look at the components, sounds good! [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                `/include/ssp/stdio.h` check the contents of that file. The path is strange for starters\n",
            "tokenize:             ['`', '/', 'include', '/', 'ssp', '/', 'stdio', '.', 'h', '`', 'check', 'the', 'contents', 'of', 'that', 'file', '.', 'The', 'path', 'is', 'strange', 'for', 'starters']\n",
            "convert_tokens_to_ids [68, 19, 3005, 19, 52731, 19, 19080, 18, 76, 68, 2175, 1783, 3881, 1815, 1826, 1973, 18, 1911, 2643, 1804, 5022, 1829, 17596]\n",
            "add special tokens:   [2, 68, 19, 3005, 19, 52731, 19, 19080, 18, 76, 68, 2175, 1783, 3881, 1815, 1826, 1973, 18, 1911, 2643, 1804, 5022, 1829, 17596, 4]\n",
            "================\n",
            "decode:               [CLS] ` / include / ssp / stdio. h ` check the contents of that file. The path is strange for starters [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                \"At each step you multiply the value of the filter 'pixel' by the corresponding value of the image pixel which is under that particular filter 'pixel' (the 9 pixels under the filter are all affected)\" I found this statement in a lot of material on convolution. I previously asked a question about this in SO and I find this to be a bit ambiguous, do we need to multiply the pixel value by the filter or take the intensity of the pixel color and then multiply the filter like in my code. if u have a non balanced filter I found that it leads to bleeding effect like the greens would be effecting red's\n",
            "tokenize:             ['\"', 'At', 'each', 'step', 'you', 'multiply', 'the', 'value', 'of', 'the', 'filter', \"'\", 'pixel', \"'\", 'by', 'the', 'corresponding', 'value', 'of', 'the', 'image', 'pixel', 'which', 'is', 'under', 'that', 'particular', 'filter', \"'\", 'pixel', \"'\", '(', 'the', '9', 'pixels', 'under', 'the', 'filter', 'are', 'all', 'affected', ')', '\"', 'I', 'found', 'this', 'statement', 'in', 'a', 'lot', 'of', 'material', 'on', 'convolution', '.', 'I', 'previously', 'asked', 'a', 'question', 'about', 'this', 'in', 'SO', 'and', 'I', 'find', 'this', 'to', 'be', 'a', 'bit', 'ambiguous', ',', 'do', 'we', 'need', 'to', 'multiply', 'the', 'pixel', 'value', 'by', 'the', 'filter', 'or', 'take', 'the', 'intensity', 'of', 'the', 'pixel', 'color', 'and', 'then', 'multiply', 'the', 'filter', 'like', 'in', 'my', 'code', '.', 'if', 'u', 'have', 'a', 'non', 'balanced', 'filter', 'I', 'found', 'that', 'it', 'leads', 'to', 'bleeding', 'effect', 'like', 'the', 'green', '##s', 'would', 'be', 'effecting', 'red', \"'\", 's']\n",
            "convert_tokens_to_ids [6, 3450, 2184, 2974, 1807, 9174, 1783, 2116, 1815, 1783, 3079, 11, 4393, 11, 1956, 1783, 4608, 2116, 1815, 1783, 2347, 4393, 1945, 1804, 2348, 1826, 3240, 3079, 11, 4393, 11, 12, 1783, 29, 5817, 2348, 1783, 3079, 1894, 1935, 8596, 13, 6, 45, 2447, 1832, 3096, 1797, 69, 2751, 1815, 7853, 1824, 19438, 18, 45, 5861, 4872, 69, 2318, 2201, 1832, 1797, 4165, 1809, 45, 2202, 1832, 1790, 1827, 69, 2504, 12096, 16, 1845, 1955, 1941, 1790, 9174, 1783, 4393, 2116, 1956, 1783, 3079, 1876, 2725, 1783, 21408, 1815, 1783, 4393, 3371, 1809, 1991, 9174, 1783, 3079, 1923, 1797, 1887, 1925, 18, 1909, 89, 1851, 69, 3019, 14776, 3079, 45, 2447, 1826, 1813, 7475, 1790, 32843, 3372, 1923, 1783, 6778, 1016, 1961, 1827, 31499, 2646, 11, 87]\n",
            "add special tokens:   [2, 6, 3450, 2184, 2974, 1807, 9174, 1783, 2116, 1815, 1783, 3079, 11, 4393, 11, 1956, 1783, 4608, 2116, 1815, 1783, 2347, 4393, 1945, 1804, 2348, 1826, 3240, 3079, 11, 4393, 11, 12, 1783, 29, 5817, 2348, 1783, 3079, 1894, 1935, 8596, 13, 6, 45, 2447, 1832, 3096, 1797, 69, 2751, 1815, 7853, 1824, 19438, 18, 45, 5861, 4872, 69, 2318, 2201, 1832, 1797, 4165, 1809, 45, 2202, 1832, 1790, 1827, 69, 2504, 12096, 16, 1845, 1955, 1941, 1790, 9174, 1783, 4393, 2116, 1956, 1783, 3079, 1876, 2725, 1783, 21408, 1815, 1783, 4393, 3371, 1809, 1991, 9174, 1783, 3079, 1923, 1797, 1887, 1925, 18, 1909, 89, 1851, 69, 3019, 14776, 3079, 45, 2447, 1826, 1813, 7475, 1790, 32843, 3372, 1923, 1783, 6778, 1016, 1961, 1827, 31499, 2646, 11, 87, 4]\n",
            "================\n",
            "decode:               [CLS] \" At each step you multiply the value of the filter'pixel'by the corresponding value of the image pixel which is under that particular filter'pixel'( the 9 pixels under the filter are all affected ) \" I found this statement in a lot of material on convolution. I previously asked a question about this in SO and I find this to be a bit ambiguous, do we need to multiply the pixel value by the filter or take the intensity of the pixel color and then multiply the filter like in my code. if u have a non balanced filter I found that it leads to bleeding effect like the greens would be effecting red's [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                Everyone suggesting STRING_SPLIT, how can this function split string into *columns* (not rows like it's intended)?\n",
            "tokenize:             ['Everyone', 'suggesting', 'STRING', '_', 'SPLIT', ',', 'how', 'can', 'this', 'function', 'split', 'string', 'into', '*', 'columns', '*', '(', 'not', 'rows', 'like', 'it', \"'\", 's', 'intended', ')', '?']\n",
            "convert_tokens_to_ids [17901, 12243, 15352, 67, 46528, 16, 1995, 1847, 1832, 2018, 4002, 2204, 2110, 14, 3175, 14, 12, 1859, 3058, 1923, 1813, 11, 87, 5787, 13, 35]\n",
            "add special tokens:   [2, 17901, 12243, 15352, 67, 46528, 16, 1995, 1847, 1832, 2018, 4002, 2204, 2110, 14, 3175, 14, 12, 1859, 3058, 1923, 1813, 11, 87, 5787, 13, 35, 4]\n",
            "================\n",
            "decode:               [CLS] Everyone suggesting STRING _ SPLIT, how can this function split string into * columns * ( not rows like it's intended )? [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                I would suggest you to put a flag there, and change that flag whenever you need.\n",
            "tokenize:             ['I', 'would', 'suggest', 'you', 'to', 'put', 'a', 'flag', 'there', ',', 'and', 'change', 'that', 'flag', 'whenever', 'you', 'need', '.']\n",
            "convert_tokens_to_ids [45, 1961, 2626, 1807, 1790, 2489, 69, 4121, 1965, 16, 1809, 2285, 1826, 4121, 4672, 1807, 1941, 18]\n",
            "add special tokens:   [2, 45, 1961, 2626, 1807, 1790, 2489, 69, 4121, 1965, 16, 1809, 2285, 1826, 4121, 4672, 1807, 1941, 18, 4]\n",
            "================\n",
            "decode:               [CLS] I would suggest you to put a flag there, and change that flag whenever you need. [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                Well, the exclusion in subqueies is documented as well, so I'd say it's   awkward, but not surprising. The reason IMO is that the whole query must guarantie to deliver identical `NEXTVAL` in all calls; which is trivial in `select seq.nextval a, seq.nextval b from dual` , but less trivial in   `select * from (select seq.nextval a from dual),(select seq.nextval b from dual)`\n",
            "tokenize:             ['Well', ',', 'the', 'exclusion', 'in', 'sub', '##que', '##ies', 'is', 'documented', 'as', 'well', ',', 'so', 'I', \"'\", 'd', 'say', 'it', \"'\", 's', 'awkward', ',', 'but', 'not', 'surprising', '.', 'The', 'reason', 'IMO', 'is', 'that', 'the', 'whole', 'query', 'must', 'guarant', '##ie', 'to', 'deliver', 'identical', '`', 'NEXT', '##VAL', '`', 'in', 'all', 'calls', ';', 'which', 'is', 'trivial', 'in', '`', 'select', 'seq', '.', 'nextval', 'a', ',', 'seq', '.', 'nextval', 'b', 'from', 'dual', '`', ',', 'but', 'less', 'trivial', 'in', '`', 'select', '*', 'from', '(', 'select', 'seq', '.', 'nextval', 'a', 'from', 'dual', ')', ',', '(', 'select', 'seq', '.', 'nextval', 'b', 'from', 'dual', ')', '`']\n",
            "convert_tokens_to_ids [4778, 16, 1783, 20020, 1797, 2307, 3153, 2119, 1804, 6986, 1856, 2542, 16, 1948, 45, 11, 72, 2620, 1813, 11, 87, 14374, 16, 1868, 1859, 11643, 18, 1911, 2691, 10706, 1804, 1826, 1783, 3289, 2400, 2743, 5047, 1940, 1790, 7717, 6178, 68, 24625, 13666, 68, 1797, 1935, 3156, 31, 1945, 1804, 6530, 1797, 68, 2357, 12772, 18, 45719, 69, 16, 12772, 18, 45719, 70, 1912, 14610, 68, 16, 1868, 3425, 6530, 1797, 68, 2357, 14, 1912, 12, 2357, 12772, 18, 45719, 69, 1912, 14610, 13, 16, 12, 2357, 12772, 18, 45719, 70, 1912, 14610, 13, 68]\n",
            "add special tokens:   [2, 4778, 16, 1783, 20020, 1797, 2307, 3153, 2119, 1804, 6986, 1856, 2542, 16, 1948, 45, 11, 72, 2620, 1813, 11, 87, 14374, 16, 1868, 1859, 11643, 18, 1911, 2691, 10706, 1804, 1826, 1783, 3289, 2400, 2743, 5047, 1940, 1790, 7717, 6178, 68, 24625, 13666, 68, 1797, 1935, 3156, 31, 1945, 1804, 6530, 1797, 68, 2357, 12772, 18, 45719, 69, 16, 12772, 18, 45719, 70, 1912, 14610, 68, 16, 1868, 3425, 6530, 1797, 68, 2357, 14, 1912, 12, 2357, 12772, 18, 45719, 69, 1912, 14610, 13, 16, 12, 2357, 12772, 18, 45719, 70, 1912, 14610, 13, 68, 4]\n",
            "================\n",
            "decode:               [CLS] Well, the exclusion in subqueies is documented as well, so I'd say it's awkward, but not surprising. The reason IMO is that the whole query must guarantie to deliver identical ` NEXTVAL ` in all calls ; which is trivial in ` select seq. nextval a, seq. nextval b from dual `, but less trivial in ` select * from ( select seq. nextval a from dual ), ( select seq. nextval b from dual ) ` [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                @NimChimpsky I have gone through that question already. It didn't help.\n",
            "tokenize:             ['@', 'Nim', '##Chimp', '##sky', 'I', 'have', 'gone', 'through', 'that', 'question', 'already', '.', 'It', 'didn', \"'\", 't', 'help', '.']\n",
            "convert_tokens_to_ids [36, 30500, 33515, 55917, 45, 1851, 6259, 2462, 1826, 2318, 2601, 18, 2049, 3186, 11, 88, 2213, 18]\n",
            "add special tokens:   [2, 36, 30500, 33515, 55917, 45, 1851, 6259, 2462, 1826, 2318, 2601, 18, 2049, 3186, 11, 88, 2213, 18, 4]\n",
            "================\n",
            "decode:               [CLS] @ NimChimpsky I have gone through that question already. It didn't help. [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                that is via Client Object Model not the Restful Api which I was asking about\n",
            "tokenize:             ['that', 'is', 'via', 'Client', 'Object', 'Model', 'not', 'the', 'Restful', 'Api', 'which', 'I', 'was', 'asking', 'about']\n",
            "convert_tokens_to_ids [1826, 1804, 2781, 5030, 3454, 4501, 1859, 1783, 26017, 10105, 1945, 45, 2078, 4465, 2201]\n",
            "add special tokens:   [2, 1826, 1804, 2781, 5030, 3454, 4501, 1859, 1783, 26017, 10105, 1945, 45, 2078, 4465, 2201, 4]\n",
            "================\n",
            "decode:               [CLS] that is via Client Object Model not the Restful Api which I was asking about [SEP]\n",
            "\n",
            "\n",
            "\n",
            "start:                Don't cast the result of a call to `malloc()` (or `calloc()` or `realloc()`) - it's unnecessary and can mask a very real error if you fail to have the appropriate prototype in scope.\n",
            "tokenize:             ['Don', \"'\", 't', 'cast', 'the', 'result', 'of', 'a', 'call', 'to', '`', 'malloc', '(', ')', '`', '(', 'or', '`', 'calloc', '(', ')', '`', 'or', '`', 'realloc', '(', ')', '`', ')', '-', 'it', \"'\", 's', 'unnecessary', 'and', 'can', 'mask', 'a', 'very', 'real', 'error', 'if', 'you', 'fail', 'to', 'have', 'the', 'appropriate', 'prototype', 'in', 'scope', '.']\n",
            "convert_tokens_to_ids [4905, 11, 88, 4644, 1783, 2279, 1815, 69, 2077, 1790, 68, 7507, 12, 13, 68, 12, 1876, 68, 22967, 12, 13, 68, 1876, 68, 13762, 12, 13, 68, 13, 17, 1813, 11, 87, 7292, 1809, 1847, 7309, 69, 2441, 2984, 2093, 1909, 1807, 3607, 1790, 1851, 1783, 4116, 6264, 1797, 3773, 18]\n",
            "add special tokens:   [2, 4905, 11, 88, 4644, 1783, 2279, 1815, 69, 2077, 1790, 68, 7507, 12, 13, 68, 12, 1876, 68, 22967, 12, 13, 68, 1876, 68, 13762, 12, 13, 68, 13, 17, 1813, 11, 87, 7292, 1809, 1847, 7309, 69, 2441, 2984, 2093, 1909, 1807, 3607, 1790, 1851, 1783, 4116, 6264, 1797, 3773, 18, 4]\n",
            "================\n",
            "decode:               [CLS] Don't cast the result of a call to ` malloc ( ) ` ( or ` calloc ( ) ` or ` realloc ( ) ` ) - it's unnecessary and can mask a very real error if you fail to have the appropriate prototype in scope. [SEP]\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t0 = df[\"text\"][0]\n",
        "tokenizer.encode(t0, return_tensors = \"pt\")\n",
        "model."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UAweK_A260gk",
        "outputId": "01e81752-fcd2-4a73-c335-09be1e0ebfed"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TokenClassifierOutput(loss=None, logits=tensor([[[-0.6075,  1.1530, -0.6116,  ...,  0.3006, -0.6447,  0.1319],\n",
            "         [-0.0289,  1.5941, -0.6061,  ...,  0.8019, -0.4946, -0.2197],\n",
            "         [-0.5585,  1.2193,  0.0123,  ...,  0.0702, -0.0290,  0.7848],\n",
            "         ...,\n",
            "         [-0.3575,  0.2732, -0.6512,  ...,  0.9173, -0.8593, -0.1063],\n",
            "         [-0.6911,  1.1412, -0.5137,  ...,  0.7186, -0.4854,  0.0965],\n",
            "         [-1.2088,  0.3238, -0.3494,  ...,  0.8452, -0.2732,  0.0849]]],\n",
            "       grad_fn=<ViewBackward0>), hidden_states=(tensor([[[ 5.2508e-02,  1.0643e+00, -1.6286e+00,  ..., -2.1369e-01,\n",
            "          -1.1551e+00,  8.0956e-01],\n",
            "         [ 1.4543e-01,  6.2975e-01, -1.4977e+00,  ...,  6.4980e-01,\n",
            "          -8.5041e-01,  7.1804e-01],\n",
            "         [-8.8615e-01,  1.7793e+00, -4.2127e-01,  ...,  4.9867e-01,\n",
            "          -1.2349e+00, -8.6165e-01],\n",
            "         ...,\n",
            "         [ 1.9428e+00, -1.0674e+00, -1.7318e-01,  ...,  3.8954e-01,\n",
            "           1.3849e-01,  1.3046e+00],\n",
            "         [ 9.9077e-01,  1.1217e+00, -2.3539e+00,  ...,  2.9621e-01,\n",
            "          -2.1999e-03, -4.7190e-01],\n",
            "         [ 1.7316e+00,  6.0886e-02,  2.5035e-01,  ...,  3.6146e-01,\n",
            "          -6.2932e-01, -1.2495e+00]]], grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.0528,  1.0101, -1.0077,  ...,  0.0399, -0.9664,  1.5010],\n",
            "         [-0.2186,  0.7444, -0.8648,  ...,  1.2452, -0.3939,  1.2597],\n",
            "         [-0.9100,  1.8520, -0.1388,  ...,  0.9766, -0.4038, -0.3743],\n",
            "         ...,\n",
            "         [ 1.8910, -1.2575, -0.0225,  ...,  0.3606,  0.8506,  1.4021],\n",
            "         [ 0.6165,  1.3029, -1.8707,  ...,  0.4699,  0.4184, -0.1166],\n",
            "         [ 1.4520,  0.2991,  0.6242,  ...,  1.1633, -0.0990, -1.0515]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0962,  0.0471, -0.7719,  ...,  0.3996, -0.5203,  1.5649],\n",
            "         [ 0.0561,  0.0025, -0.6329,  ...,  1.2393, -0.0886,  1.0406],\n",
            "         [-0.8147,  1.3205,  0.1956,  ...,  0.9215, -0.1392, -0.1762],\n",
            "         ...,\n",
            "         [ 1.3539, -1.3733,  0.3610,  ..., -0.1052,  0.7352,  1.7893],\n",
            "         [ 0.5565,  0.6881, -1.0323,  ...,  0.6608, -0.1174,  0.1970],\n",
            "         [ 2.2520,  0.2721,  1.5719,  ...,  1.2371,  0.0195, -0.8818]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.2275,  0.2504, -0.7596,  ..., -0.1987, -0.4775,  1.6700],\n",
            "         [ 0.5035,  0.0576, -0.5636,  ...,  1.1600, -0.0408,  1.0179],\n",
            "         [-0.6378,  1.4206,  0.4993,  ...,  1.0227, -0.1438, -0.3936],\n",
            "         ...,\n",
            "         [ 1.4605, -0.8076, -0.0573,  ...,  0.0170,  0.1074,  1.5667],\n",
            "         [ 0.9260,  1.0920, -0.7360,  ...,  0.9281, -0.2888, -0.3498],\n",
            "         [ 2.1404,  0.3090,  1.5479,  ...,  1.2880,  0.1807, -0.7383]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[ 0.3844, -0.1521, -0.9600,  ...,  0.2590,  0.0984,  1.8015],\n",
            "         [ 0.9330, -0.2523, -1.5140,  ...,  1.5170,  0.5307,  1.5369],\n",
            "         [-0.3717,  1.0831,  0.3765,  ...,  1.2979,  0.7109, -0.1296],\n",
            "         ...,\n",
            "         [ 1.7214, -1.1024, -0.2940,  ...,  0.1776,  0.3874,  1.5657],\n",
            "         [ 1.5603,  0.8394, -0.4207,  ...,  1.0592,  0.0092, -0.6467],\n",
            "         [ 2.0016, -0.2947,  1.6834,  ...,  1.6792,  0.9672, -0.6250]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0517, -0.2196, -1.2048,  ..., -0.0275,  0.8319,  1.7200],\n",
            "         [ 0.7374, -0.5500, -0.7575,  ...,  1.1626,  0.5948,  1.7111],\n",
            "         [-0.6018,  1.0013,  0.7887,  ...,  1.0881,  0.5756,  0.5325],\n",
            "         ...,\n",
            "         [ 0.7015, -1.2420,  0.1630,  ...,  0.0534,  1.0824,  1.4409],\n",
            "         [ 1.1881,  0.8111,  0.2068,  ...,  0.7327,  0.5500, -0.3289],\n",
            "         [ 1.1761, -0.4457,  1.4043,  ...,  1.4305,  1.0960, -0.8259]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.7023, -0.4198, -1.8297,  ...,  0.3593, -0.0351,  1.2780],\n",
            "         [ 0.2687, -0.9581, -1.3127,  ...,  1.4842, -0.1012,  1.0911],\n",
            "         [-0.9362,  1.2365,  0.1295,  ...,  1.3159, -0.1991,  0.4589],\n",
            "         ...,\n",
            "         [ 0.4142, -0.9605, -0.8461,  ...,  0.0186,  0.1514,  1.4435],\n",
            "         [ 1.1009,  1.1692, -0.3774,  ...,  0.9361, -0.0029, -0.6940],\n",
            "         [ 0.7772, -0.4110,  0.8451,  ...,  1.4640,  0.4175, -0.6932]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.9349, -0.2543, -1.7654,  ..., -0.0318, -0.7986,  0.9973],\n",
            "         [-0.5166, -0.9382, -1.2588,  ...,  1.4471, -0.5404,  1.1399],\n",
            "         [-1.3519,  1.1212,  0.3503,  ...,  0.9996, -0.9401,  0.5235],\n",
            "         ...,\n",
            "         [ 0.3998, -0.7362, -0.5265,  ..., -0.0122, -0.2827,  1.4435],\n",
            "         [ 0.5452,  1.4361, -0.3384,  ...,  0.8019, -0.3380, -0.6088],\n",
            "         [ 0.4854, -0.0671,  0.5776,  ...,  1.0027,  0.0983, -0.6108]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.5658, -0.2160, -1.8195,  ...,  0.9483, -0.6394,  1.1255],\n",
            "         [ 0.2213, -0.5136, -1.1146,  ...,  1.9157, -0.6872,  1.2720],\n",
            "         [-1.3234,  1.8600, -0.1005,  ...,  1.1328, -1.5944,  0.3608],\n",
            "         ...,\n",
            "         [ 0.5194, -0.4055, -0.8330,  ...,  0.3427, -0.7986,  1.2846],\n",
            "         [ 1.1859,  1.0738, -0.2721,  ...,  1.2717, -0.6287, -0.7050],\n",
            "         [ 0.9503,  0.4339,  0.0976,  ...,  1.0311,  0.0201, -0.6324]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0427, -0.2875, -1.3545,  ...,  1.0212, -0.9026,  0.5758],\n",
            "         [ 0.9549, -1.2665, -1.0005,  ...,  2.1358, -1.1161,  1.0643],\n",
            "         [-0.8432,  1.3847, -0.3474,  ...,  1.8959, -1.9668,  0.3854],\n",
            "         ...,\n",
            "         [ 1.0342, -0.6033, -0.4800,  ...,  0.0612, -0.9601,  1.4662],\n",
            "         [ 2.3231,  0.4358,  0.1270,  ...,  1.3278, -0.6949, -0.7710],\n",
            "         [ 1.1944,  0.0646,  0.1604,  ...,  1.1354, -0.2743, -0.6496]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.0868, -0.5747, -0.5679,  ...,  1.7973, -1.4415,  0.1949],\n",
            "         [ 0.6345, -0.7826, -0.2981,  ...,  2.6278, -1.0146,  0.9163],\n",
            "         [-0.9052,  1.2459, -0.1700,  ...,  2.8946, -1.7816,  0.4649],\n",
            "         ...,\n",
            "         [ 0.9274, -0.6557,  0.0154,  ...,  1.1793, -0.9389,  1.2333],\n",
            "         [ 2.2671,  0.3720,  0.5185,  ...,  1.9304, -0.7812, -0.5852],\n",
            "         [ 1.3267,  0.1476,  0.9420,  ...,  1.5308, -0.4650, -0.3196]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.3387, -0.0808, -0.8314,  ...,  1.1872, -1.4531,  0.2685],\n",
            "         [ 0.5653, -0.6326, -0.7183,  ...,  2.3897, -1.1360,  0.6159],\n",
            "         [-0.9902,  1.0795, -0.3144,  ...,  2.4871, -1.4277,  0.0885],\n",
            "         ...,\n",
            "         [ 0.8555, -0.2516,  0.0132,  ...,  1.1142, -1.0475,  1.1126],\n",
            "         [ 1.8110,  0.5650,  0.5429,  ...,  1.9192, -0.8009, -0.7777],\n",
            "         [ 1.2224,  0.7948,  0.7224,  ...,  1.4499, -0.7357, -0.6378]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>), tensor([[[-0.2050, -0.0688, -0.5645,  ...,  0.5987, -1.0958,  0.7958],\n",
            "         [ 0.3425, -0.3913, -0.3331,  ...,  2.1010, -0.3937,  0.5108],\n",
            "         [-1.0571,  1.0755, -0.0831,  ...,  2.0804, -0.9526,  0.4214],\n",
            "         ...,\n",
            "         [ 0.8547,  0.5957, -0.2068,  ...,  0.8624, -0.8178,  1.5098],\n",
            "         [ 1.4499,  0.5671,  0.4959,  ...,  1.7280, -0.0436, -0.3490],\n",
            "         [ 0.9346,  1.3205,  0.3664,  ...,  1.0946, -0.4618, -0.0500]]],\n",
            "       grad_fn=<NativeLayerNormBackward0>)), attentions=None)\n"
          ]
        }
      ]
    }
  ]
}